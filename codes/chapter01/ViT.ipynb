{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8eb615-6780-4cf7-9cc5-d37d8aebf66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46dc6dfc-c026-42f5-8be9-06f93b24f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,    # 模型的维度\n",
    "        img_size,   # 图片大小\n",
    "        patch_size, # 补丁大小\n",
    "        n_channels  # 通道数量\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        self.linear_project = nn.Conv2d(\n",
    "            self.n_channels, # in_channels\n",
    "            self.d_model, # out_channels\n",
    "            kernel_size=self.patch_size, # kernel_size\n",
    "            stride=self.patch_size # stride\n",
    "        )\n",
    "\n",
    "    # B: 批次大小\n",
    "    # C: 通道数量\n",
    "    # H: 图像高度\n",
    "    # W: 图像宽度\n",
    "    # P_col: 补丁的列\n",
    "    # P_row: 补丁的行\n",
    "    def forward(self, x):\n",
    "        x = self.linear_project(x) # (B, C, H, W) -> (B, d_model, P_col, P_row)\n",
    "        x = x.flatten(2) # (B, d_model, P_col, P_row) -> (B, d_model, P)\n",
    "        x = x.transpose(1, 2) # (B, d_model, P) -> (B, P, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b493767-decd-4803-b422-07aac99aafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super().__init__()\n",
    "        # 类别token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        # 创建位置编码\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(d_model):\n",
    "                if i % 2 == 0:\n",
    "                    pe[pos][i] = np.sin(pos/(10000 ** (i/d_model)))\n",
    "                else:\n",
    "                    pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/d_model)))\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 为批次中的每张图片分配一个类别token\n",
    "        tokens_batch = self.cls_token.expand(x.size()[0], -1, -1)\n",
    "        # 将类别token添加到每个嵌入的开头\n",
    "        x = torch.cat((tokens_batch,x), dim=1)\n",
    "        # 将位置编码添加到嵌入中\n",
    "        x = x + self.pe\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e5cbea-266c-4217-b297-0e23aa004a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.query = nn.Linear(d_model, head_size)\n",
    "        self.key = nn.Linear(d_model, head_size)\n",
    "        self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 计算Q, K, V\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Q和K的点积\n",
    "        attention = Q @ K.transpose(-2,-1)\n",
    "\n",
    "        # 缩放\n",
    "        attention = attention / (self.head_size ** 0.5)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        attention = attention @ V\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f7c975-3d1b-4d33-882b-07e5d325281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = d_model // n_heads\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, self.head_size) for _ in range(n_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 拼接多个注意力头\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.W_o(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9d5fd6-23c5-49ec-ab7e-13b9c6d28f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 层归一化\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 多头注意力\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "        # 层归一化\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model*r_mlp, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 第一次层归一化之后的残差\n",
    "        out = x + self.mha(self.ln1(x))\n",
    "        # 第二次层归一化之后的残差\n",
    "        out = out + self.mlp(self.ln2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07956a46-a9be-4483-a44f-1bbe9764d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        n_classes,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        n_channels,\n",
    "        n_heads,\n",
    "        n_layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size[0] % patch_size[0] == 0  \\\n",
    "           and img_size[1] % patch_size[1] == 0, \\\n",
    "           \"img_size 必须能被 patch_size 整除\"\n",
    "        assert d_model % n_heads == 0, \\\n",
    "           \"d_model 必须能被 n_heads 整除\"\n",
    "\n",
    "        self.d_model = d_model # 模型维度\n",
    "        self.n_classes = n_classes # 类别的数量\n",
    "        self.img_size = img_size # 图片大小\n",
    "        self.patch_size = patch_size # 补丁大小\n",
    "        self.n_channels = n_channels # 通道数\n",
    "        self.n_heads = n_heads # 注意力头的数量\n",
    "\n",
    "        self.n_patches = (self.img_size[0] * self.img_size[1]) \\\n",
    "                      // (self.patch_size[0] * self.patch_size[1])\n",
    "        self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            self.d_model,\n",
    "            self.img_size,\n",
    "            self.patch_size,\n",
    "            self.n_channels\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            self.d_model,\n",
    "            self.max_seq_length\n",
    "        )\n",
    "        self.transformer_encoder = nn.Sequential(*[\n",
    "            TransformerEncoder(self.d_model, self.n_heads)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # 用于分类的MLP\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.n_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.patch_embedding(images)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.classifier(x[:,0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98f2958-9314-4177-b8bb-7cb0663d6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 9\n",
    "n_classes = 10\n",
    "img_size = (32,32)\n",
    "patch_size = (16,16)\n",
    "n_channels = 1\n",
    "n_heads = 3\n",
    "n_layers = 3\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f2f9b93-3b84-4544-a2c9-b6204b687eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize(img_size),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_set = MNIST(\n",
    "    root=\"./../datasets\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_set = MNIST(\n",
    "    root=\"./../datasets\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "317b2c94-fbf2-4c5f-93d2-9e07db50819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 1.726\n",
      "Epoch 2/5 loss: 1.580\n",
      "Epoch 3/5 loss: 1.558\n",
      "Epoch 4/5 loss: 1.550\n",
      "Epoch 5/5 loss: 1.545\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "ViT = VisionTransformer(\n",
    "    d_model,\n",
    "    n_classes,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    n_heads,\n",
    "    n_layers\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(ViT.parameters(), lr=alpha)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    training_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = ViT(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss  / len(train_loader) :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "773c563d-0b6b-466e-8ee7-9e21d03b4617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "预测准确率: 92 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = ViT(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'\\n预测准确率: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a1fda-8240-457a-903d-37567c671b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
