## ğŸ¯ ç†è®ºåŸºç¡€

### é«˜æ–¯åˆ†å¸ƒçš„å®šä¹‰

é«˜æ–¯åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰æ˜¯ç»Ÿè®¡å­¦ä¸­æœ€é‡è¦çš„è¿ç»­æ¦‚ç‡åˆ†å¸ƒä¹‹ä¸€ï¼Œç”±å¾·å›½æ•°å­¦å®¶é«˜æ–¯æå‡ºã€‚

#### ä¸€å…ƒé«˜æ–¯åˆ†å¸ƒ

$$
f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$

å…¶ä¸­ï¼š
- $\mu$ : å‡å€¼ï¼ˆæœŸæœ›å€¼ï¼‰
- $\sigma^2$ : æ–¹å·®
- $\sigma$ : æ ‡å‡†å·®

#### äºŒå…ƒé«˜æ–¯åˆ†å¸ƒ

$$
f(x,y|\boldsymbol{\mu},\boldsymbol{\Sigma}) = \frac{1}{2\pi\sqrt{|\boldsymbol{\Sigma}|}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)
$$

å…¶ä¸­ï¼š
- $\boldsymbol{\mu} = [\mu_x, \mu_y]^T$ : å‡å€¼å‘é‡
- $\boldsymbol{\Sigma}$ : åæ–¹å·®çŸ©é˜µ
- $|\boldsymbol{\Sigma}|$ : åæ–¹å·®çŸ©é˜µçš„è¡Œåˆ—å¼

### åæ–¹å·®çŸ©é˜µ

$$
\boldsymbol{\Sigma} = \begin{bmatrix}
\sigma_x^2 & \sigma_{xy} \\
\sigma_{xy} & \sigma_y^2
\end{bmatrix}
$$

å…¶ä¸­ï¼š
- $\sigma_x^2, \sigma_y^2$ : xå’Œyçš„æ–¹å·®
- $\sigma_{xy}$ : xå’Œyçš„åæ–¹å·®

## ğŸ“ˆ ä¸€å…ƒé«˜æ–¯åˆ†å¸ƒ

### åŸºç¡€å®ç°å’Œå¯è§†åŒ–

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

def plot_univariate_gaussian():
    """ä¸€å…ƒé«˜æ–¯åˆ†å¸ƒå¯è§†åŒ–"""
    x = np.linspace(-10, 10, 1000)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ä¸€å…ƒé«˜æ–¯åˆ†å¸ƒç‰¹æ€§åˆ†æ', fontsize=16, fontweight='bold')
    
    # 1. ä¸åŒå‡å€¼ï¼Œç›¸åŒæ–¹å·®
    ax1 = axes[0, 0]
    means = [0, 2, -2]
    variance = 1
    colors = ['blue', 'red', 'green']
    
    for mu, color in zip(means, colors):
        y = norm.pdf(x, mu, np.sqrt(variance))
        ax1.plot(x, y, color=color, linewidth=2, 
                label=f'Î¼={mu}, ÏƒÂ²={variance}')
        ax1.axvline(mu, color=color, linestyle='--', alpha=0.7)
    
    ax1.set_title('ä¸åŒå‡å€¼çš„å½±å“ (ÏƒÂ² = 1)', fontweight='bold')
    ax1.set_xlabel('x')
    ax1.set_ylabel('æ¦‚ç‡å¯†åº¦ f(x)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ç›¸åŒå‡å€¼ï¼Œä¸åŒæ–¹å·®
    ax2 = axes[0, 1]
    mu = 0
    variances = [0.5, 1, 2, 4]
    colors = ['purple', 'blue', 'orange', 'red']
    
    for var, color in zip(variances, colors):
        y = norm.pdf(x, mu, np.sqrt(var))
        ax2.plot(x, y, color=color, linewidth=2, 
                label=f'Î¼={mu}, ÏƒÂ²={var}')
    
    ax2.set_title('ä¸åŒæ–¹å·®çš„å½±å“ (Î¼ = 0)', fontweight='bold')
    ax2.set_xlabel('x')
    ax2.set_ylabel('æ¦‚ç‡å¯†åº¦ f(x)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (CDF)
    ax3 = axes[1, 0]
    mu, sigma = 0, 1
    y_pdf = norm.pdf(x, mu, sigma)
    y_cdf = norm.cdf(x, mu, sigma)
    
    ax3.plot(x, y_pdf, 'b-', linewidth=2, label='æ¦‚ç‡å¯†åº¦å‡½æ•° (PDF)')
    ax3.plot(x, y_cdf, 'r-', linewidth=2, label='ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (CDF)')
    ax3.axhline(0.5, color='gray', linestyle='--', alpha=0.7)
    ax3.axvline(0, color='gray', linestyle='--', alpha=0.7)
    
    ax3.set_title('PDF vs CDF (æ ‡å‡†æ­£æ€åˆ†å¸ƒ)', fontweight='bold')
    ax3.set_xlabel('x')
    ax3.set_ylabel('æ¦‚ç‡')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. 68-95-99.7è§„åˆ™å¯è§†åŒ–
    ax4 = axes[1, 1]
    mu, sigma = 0, 1
    x_fine = np.linspace(-4, 4, 1000)
    y = norm.pdf(x_fine, mu, sigma)
    ax4.plot(x_fine, y, 'b-', linewidth=2, label='N(0,1)')
    
    # å¡«å……ä¸åŒæ ‡å‡†å·®èŒƒå›´
    x_1sigma = x_fine[np.abs(x_fine) <= 1]
    y_1sigma = norm.pdf(x_1sigma, mu, sigma)
    ax4.fill_between(x_1sigma, y_1sigma, alpha=0.3, color='green', 
                     label='Â±1Ïƒ (68.27%)')
    
    x_2sigma = x_fine[np.abs(x_fine) <= 2]
    y_2sigma = norm.pdf(x_2sigma, mu, sigma)
    ax4.fill_between(x_2sigma, y_2sigma, alpha=0.2, color='orange', 
                     label='Â±2Ïƒ (95.45%)')
    
    x_3sigma = x_fine[np.abs(x_fine) <= 3]
    y_3sigma = norm.pdf(x_3sigma, mu, sigma)
    ax4.fill_between(x_3sigma, y_3sigma, alpha=0.1, color='red', 
                     label='Â±3Ïƒ (99.73%)')
    
    ax4.set_title('68-95-99.7è§„åˆ™', fontweight='bold')
    ax4.set_xlabel('x')
    ax4.set_ylabel('æ¦‚ç‡å¯†åº¦')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# è¿è¡Œå¯è§†åŒ–
plot_univariate_gaussian()
```

### æ•°å­¦æ€§è´¨éªŒè¯

```python
def verify_gaussian_properties():
    """éªŒè¯é«˜æ–¯åˆ†å¸ƒçš„æ•°å­¦æ€§è´¨"""
    print("=" * 60)
    print("ä¸€å…ƒé«˜æ–¯åˆ†å¸ƒæ•°å­¦æ€§è´¨éªŒè¯")
    print("=" * 60)
    
    # ç”Ÿæˆæ ·æœ¬
    mu, sigma = 2, 1.5
    samples = np.random.normal(mu, sigma, 100000)
    
    # 1. å‡å€¼å’Œæ–¹å·®
    sample_mean = np.mean(samples)
    sample_var = np.var(samples)
    
    print(f"ç†è®ºå‡å€¼: {mu:.3f}")
    print(f"æ ·æœ¬å‡å€¼: {sample_mean:.3f}")
    print(f"è¯¯å·®: {abs(mu - sample_mean):.6f}")
    print()
    
    print(f"ç†è®ºæ–¹å·®: {sigma**2:.3f}")
    print(f"æ ·æœ¬æ–¹å·®: {sample_var:.3f}")
    print(f"è¯¯å·®: {abs(sigma**2 - sample_var):.6f}")
    print()
    
    # 2. æ¦‚ç‡è®¡ç®—
    # P(X â‰¤ Î¼) = 0.5
    prob_less_than_mean = norm.cdf(mu, mu, sigma)
    print(f"P(X â‰¤ Î¼) = {prob_less_than_mean:.6f} (ç†è®ºå€¼: 0.5)")
    
    # P(Î¼-Ïƒ â‰¤ X â‰¤ Î¼+Ïƒ) â‰ˆ 0.6827
    prob_1sigma = norm.cdf(mu + sigma, mu, sigma) - norm.cdf(mu - sigma, mu, sigma)
    print(f"P(Î¼-Ïƒ â‰¤ X â‰¤ Î¼+Ïƒ) = {prob_1sigma:.6f} (ç†è®ºå€¼: 0.6827)")
    
    # 3. æ¦‚ç‡å¯†åº¦å‡½æ•°éªŒè¯
    def gaussian_pdf(x, mu, sigma):
        return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)
    
    test_x = mu
    theoretical_pdf = gaussian_pdf(test_x, mu, sigma)
    scipy_pdf = norm.pdf(test_x, mu, sigma)
    
    print(f"\nåœ¨x={test_x}å¤„çš„æ¦‚ç‡å¯†åº¦:")
    print(f"æ‰‹å·¥è®¡ç®—: {theoretical_pdf:.6f}")
    print(f"SciPyè®¡ç®—: {scipy_pdf:.6f}")
    print(f"è¯¯å·®: {abs(theoretical_pdf - scipy_pdf):.10f}")

verify_gaussian_properties()
```

## ğŸ“Š äºŒå…ƒé«˜æ–¯åˆ†å¸ƒ

### åæ–¹å·®çŸ©é˜µçš„å½±å“

```python
from scipy.stats import multivariate_normal

def plot_bivariate_gaussian():
    """äºŒå…ƒé«˜æ–¯åˆ†å¸ƒå¯è§†åŒ–"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('äºŒå…ƒé«˜æ–¯åˆ†å¸ƒç‰¹æ€§åˆ†æ', fontsize=16, fontweight='bold')
    
    # åˆ›å»ºç½‘æ ¼
    x = np.linspace(-4, 4, 100)
    y = np.linspace(-4, 4, 100)
    X, Y = np.meshgrid(x, y)
    pos = np.dstack((X, Y))
    
    # ä¸åŒçš„åæ–¹å·®çŸ©é˜µé…ç½®
    configs = [
        {'mean': [0, 0], 'cov': [[1, 0], [0, 1]], 'title': 'ç‹¬ç«‹å˜é‡\n(Ï=0)'},
        {'mean': [0, 0], 'cov': [[1, 0.8], [0.8, 1]], 'title': 'æ­£ç›¸å…³\n(Ï=0.8)'},
        {'mean': [0, 0], 'cov': [[1, -0.8], [-0.8, 1]], 'title': 'è´Ÿç›¸å…³\n(Ï=-0.8)'},
        {'mean': [0, 0], 'cov': [[2, 0], [0, 0.5]], 'title': 'ä¸åŒæ–¹å·®\n(Ïƒâ‚Â²=2, Ïƒâ‚‚Â²=0.5)'},
        {'mean': [1, -1], 'cov': [[1, 0], [0, 1]], 'title': 'ä¸åŒå‡å€¼\n(Î¼=[1,-1])'},
        {'mean': [0, 0], 'cov': [[1, 0.5], [0.5, 2]], 'title': 'ä¸€èˆ¬æƒ…å†µ\n(æ··åˆæ•ˆåº”)'}
    ]
    
    for i, config in enumerate(configs):
        row, col = i // 3, i % 3
        ax = axes[row, col]
        
        # åˆ›å»ºå¤šå…ƒæ­£æ€åˆ†å¸ƒ
        rv = multivariate_normal(config['mean'], config['cov'])
        
        # è®¡ç®—æ¦‚ç‡å¯†åº¦
        pdf_values = rv.pdf(pos)
        
        # ç»˜åˆ¶ç­‰é«˜çº¿
        contour = ax.contour(X, Y, pdf_values, levels=8, colors='black', alpha=0.6, linewidths=1)
        contourf = ax.contourf(X, Y, pdf_values, levels=20, cmap='viridis', alpha=0.8)
        
        # æ·»åŠ é¢œè‰²æ¡
        plt.colorbar(contourf, ax=ax, shrink=0.8)
        
        # æ ‡è®°å‡å€¼ç‚¹
        ax.plot(config['mean'][0], config['mean'][1], 'r*', markersize=15, 
                markeredgecolor='white', markeredgewidth=1)
        
        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title(config['title'], fontweight='bold')
        ax.set_xlabel('Xâ‚')
        ax.set_ylabel('Xâ‚‚')
        ax.grid(True, alpha=0.3)
        ax.set_aspect('equal')
    
    plt.tight_layout()
    plt.show()

plot_bivariate_gaussian()
```

### åæ–¹å·®çŸ©é˜µåˆ†è§£ä¸ç‰¹å¾å€¼

```python
def analyze_covariance_matrix():
    """åˆ†æåæ–¹å·®çŸ©é˜µçš„æ•°å­¦æ€§è´¨"""
    print("=" * 60)
    print("åæ–¹å·®çŸ©é˜µåˆ†æ")
    print("=" * 60)
    
    # å®šä¹‰å‡ ä¸ªåæ–¹å·®çŸ©é˜µ
    covariance_matrices = {
        'ç‹¬ç«‹å˜é‡': np.array([[1, 0], [0, 1]]),
        'æ­£ç›¸å…³': np.array([[1, 0.8], [0.8, 1]]),
        'è´Ÿç›¸å…³': np.array([[1, -0.8], [-0.8, 1]]),
        'ä¸åŒæ–¹å·®': np.array([[2, 0], [0, 0.5]])
    }
    
    for name, cov in covariance_matrices.items():
        print(f"\n{name}:")
        print(f"åæ–¹å·®çŸ©é˜µ:\n{cov}")
        
        # è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
        eigenvals, eigenvecs = np.linalg.eig(cov)
        print(f"ç‰¹å¾å€¼: {eigenvals}")
        print(f"ç‰¹å¾å‘é‡:\n{eigenvecs}")
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = cov[0, 1] / np.sqrt(cov[0, 0] * cov[1, 1])
        print(f"ç›¸å…³ç³»æ•° Ï: {correlation:.3f}")
        
        # è®¡ç®—è¡Œåˆ—å¼ï¼ˆç”¨äºæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼‰
        det = np.linalg.det(cov)
        print(f"è¡Œåˆ—å¼ |Î£|: {det:.3f}")
        
        print("-" * 40)

analyze_covariance_matrix()
```

### 3Då¯è§†åŒ–

```python
def plot_3d_gaussian():
    """3Då¯è§†åŒ–äºŒå…ƒé«˜æ–¯åˆ†å¸ƒ"""
    from mpl_toolkits.mplot3d import Axes3D
    
    fig = plt.figure(figsize=(15, 5))
    
    # åˆ›å»ºç½‘æ ¼
    x = np.linspace(-3, 3, 50)
    y = np.linspace(-3, 3, 50)
    X, Y = np.meshgrid(x, y)
    pos = np.dstack((X, Y))
    
    # ä¸‰ç§ä¸åŒçš„é…ç½®
    configs = [
        {'mean': [0, 0], 'cov': [[1, 0], [0, 1]], 'title': 'ç‹¬ç«‹å˜é‡ (Ï=0)'},
        {'mean': [0, 0], 'cov': [[1, 0.8], [0.8, 1]], 'title': 'æ­£ç›¸å…³ (Ï=0.8)'},
        {'mean': [0, 0], 'cov': [[1, -0.8], [-0.8, 1]], 'title': 'è´Ÿç›¸å…³ (Ï=-0.8)'}
    ]
    
    for i, config in enumerate(configs):
        ax = fig.add_subplot(1, 3, i+1, projection='3d')
        
        # åˆ›å»ºåˆ†å¸ƒå¹¶è®¡ç®—PDF
        rv = multivariate_normal(config['mean'], config['cov'])
        Z = rv.pdf(pos)
        
        # 3Dè¡¨é¢å›¾
        surf = ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, 
                              linewidth=0, antialiased=True)
        
        # æ·»åŠ ç­‰é«˜çº¿æŠ•å½±
        ax.contour(X, Y, Z, zdir='z', offset=0, cmap='viridis', alpha=0.5)
        
        ax.set_title(config['title'], fontweight='bold')
        ax.set_xlabel('Xâ‚')
        ax.set_ylabel('Xâ‚‚')
        ax.set_zlabel('æ¦‚ç‡å¯†åº¦')
    
    plt.tight_layout()
    plt.show()

plot_3d_gaussian()
```

## ğŸ¨ æ‰©æ•£æ¨¡å‹ä¸­çš„é«˜æ–¯å™ªå£°

### ä¸ºä»€ä¹ˆé€‰æ‹©é«˜æ–¯å™ªå£°ï¼Ÿ

#### 1. æ•°å­¦æ€§è´¨ä¼˜è¶Š

```python
def demonstrate_gaussian_properties():
    """æ¼”ç¤ºé«˜æ–¯åˆ†å¸ƒçš„ä¼˜è‰¯æ€§è´¨"""
    print("=" * 60)
    print("é«˜æ–¯åˆ†å¸ƒåœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„ä¼˜åŠ¿")
    print("=" * 60)
    
    # 1. ä¸­å¿ƒæé™å®šç†
    print("1. ä¸­å¿ƒæé™å®šç†éªŒè¯:")
    sample_sizes = [1, 5, 10, 30]
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    axes = axes.flatten()
    
    for i, n in enumerate(sample_sizes):
        # ä»å‡åŒ€åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç„¶åæ±‚å‡å€¼
        samples = []
        for _ in range(10000):
            uniform_samples = np.random.uniform(0, 1, n)
            samples.append(np.mean(uniform_samples))
        
        axes[i].hist(samples, bins=50, density=True, alpha=0.7, 
                    color='skyblue', edgecolor='black')
        
        # ç†è®ºæ­£æ€åˆ†å¸ƒ
        sample_mean = np.mean(samples)
        sample_std = np.std(samples)
        x = np.linspace(min(samples), max(samples), 100)
        y = norm.pdf(x, sample_mean, sample_std)
        axes[i].plot(x, y, 'r-', linewidth=2, label='ç†è®ºæ­£æ€åˆ†å¸ƒ')
        
        axes[i].set_title(f'æ ·æœ¬å¤§å° n={n}')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
    
    plt.suptitle('ä¸­å¿ƒæé™å®šç†ï¼šå‡åŒ€åˆ†å¸ƒçš„æ ·æœ¬å‡å€¼è¶‹å‘æ­£æ€åˆ†å¸ƒ', fontsize=14)
    plt.tight_layout()
    plt.show()
    
    # 2. å¯åŠ æ€§
    print("\n2. é«˜æ–¯åˆ†å¸ƒçš„å¯åŠ æ€§:")
    mu1, sigma1 = 1, 2
    mu2, sigma2 = 3, 1.5
    
    # ä¸¤ä¸ªç‹¬ç«‹é«˜æ–¯åˆ†å¸ƒçš„å’Œ
    mu_sum = mu1 + mu2
    sigma_sum = np.sqrt(sigma1**2 + sigma2**2)
    
    print(f"Xâ‚ ~ N({mu1}, {sigma1}Â²)")
    print(f"Xâ‚‚ ~ N({mu2}, {sigma2}Â²)")
    print(f"Xâ‚ + Xâ‚‚ ~ N({mu_sum}, {sigma_sum:.3f}Â²)")
    
    # éªŒè¯
    samples1 = np.random.normal(mu1, sigma1, 100000)
    samples2 = np.random.normal(mu2, sigma2, 100000)
    samples_sum = samples1 + samples2
    
    print(f"ç†è®ºå‡å€¼: {mu_sum}, å®é™…å‡å€¼: {np.mean(samples_sum):.3f}")
    print(f"ç†è®ºæ ‡å‡†å·®: {sigma_sum:.3f}, å®é™…æ ‡å‡†å·®: {np.std(samples_sum):.3f}")

demonstrate_gaussian_properties()
```

#### 2. æ‰©æ•£è¿‡ç¨‹çš„æ•°å­¦å»ºæ¨¡

$$
\begin{align}
\text{å‰å‘è¿‡ç¨‹ï¼š} \quad q(x_t|x_{t-1}) &= \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I) \\
\text{é€†å‘è¿‡ç¨‹ï¼š} \quad p_\theta(x_{t-1}|x_t) &= \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \Sigma_\theta(x_t,t))
\end{align}
$$

```python
def simulate_diffusion_process():
    """æ¨¡æ‹Ÿæ‰©æ•£è¿‡ç¨‹"""
    print("\n3. æ‰©æ•£è¿‡ç¨‹æ¨¡æ‹Ÿ:")
    
    # åŸå§‹å›¾åƒï¼ˆç®€åŒ–ä¸º1Dä¿¡å·ï¼‰
    original_signal = np.sin(np.linspace(0, 4*np.pi, 100)) + 0.5*np.cos(np.linspace(0, 8*np.pi, 100))
    
    # æ‰©æ•£å‚æ•°
    T = 1000  # æ€»æ­¥æ•°
    beta_start, beta_end = 1e-4, 0.02
    betas = np.linspace(beta_start, beta_end, T)
    alphas = 1 - betas
    alpha_bars = np.cumprod(alphas)
    
    # å‰å‘æ‰©æ•£è¿‡ç¨‹
    timesteps_to_show = [0, 100, 300, 500, 999]
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 8))
    axes = axes.flatten()
    
    for i, t in enumerate(timesteps_to_show):
        if t == 0:
            noisy_signal = original_signal
        else:
            # ç›´æ¥é‡‡æ · x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1-alpha_bar_t) * noise
            noise = np.random.normal(0, 1, len(original_signal))
            noisy_signal = np.sqrt(alpha_bars[t]) * original_signal + np.sqrt(1 - alpha_bars[t]) * noise
        
        axes[i].plot(original_signal, 'b-', alpha=0.7, label='åŸå§‹ä¿¡å·')
        axes[i].plot(noisy_signal, 'r-', alpha=0.8, label=f'å™ªå£°ä¿¡å· (t={t})')
        axes[i].set_title(f'æ—¶é—´æ­¥ t={t}\nä¿¡å™ªæ¯”: {10*np.log10(alpha_bars[t]/(1-alpha_bars[t])):.1f} dB' if t > 0 else 'åŸå§‹ä¿¡å·')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
    
    # æœ€åä¸€ä¸ªå­å›¾æ˜¾ç¤ºå™ªå£°è°ƒåº¦
    axes[-1].plot(betas, label='Î²_t (å™ªå£°è°ƒåº¦)')
    axes[-1].plot(alpha_bars, label='á¾±_t (ç´¯ç§¯ä¿¡å·ä¿æŒç‡)')
    axes[-1].set_xlabel('æ—¶é—´æ­¥')
    axes[-1].set_ylabel('å€¼')
    axes[-1].set_title('æ‰©æ•£è°ƒåº¦å‚æ•°')
    axes[-1].legend()
    axes[-1].grid(True, alpha=0.3)
    
    plt.suptitle('æ‰©æ•£è¿‡ç¨‹ï¼šä»ä¿¡å·åˆ°å™ªå£°', fontsize=14)
    plt.tight_layout()
    plt.show()

simulate_diffusion_process()
```

#### 3. é‡å‚æ•°åŒ–æŠ€å·§

```python
def demonstrate_reparameterization():
    """æ¼”ç¤ºé‡å‚æ•°åŒ–æŠ€å·§"""
    print("\n4. é‡å‚æ•°åŒ–æŠ€å·§:")
    print("ç›®æ ‡ï¼šä» N(Î¼, ÏƒÂ²) é‡‡æ ·")
    print("æ–¹æ³•ï¼šz = Î¼ + Ïƒ * Îµ, å…¶ä¸­ Îµ ~ N(0, 1)")
    
    # å‚æ•°
    mu, sigma = 2.5, 1.8
    n_samples = 10000
    
    # æ–¹æ³•1ï¼šç›´æ¥é‡‡æ ·
    samples_direct = np.random.normal(mu, sigma, n_samples)
    
    # æ–¹æ³•2ï¼šé‡å‚æ•°åŒ–
    epsilon = np.random.normal(0, 1, n_samples)  # æ ‡å‡†æ­£æ€åˆ†å¸ƒ
    samples_reparam = mu + sigma * epsilon
    
    # æ¯”è¾ƒç»“æœ
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    # æ ‡å‡†æ­£æ€åˆ†å¸ƒ Îµ
    axes[0].hist(epsilon, bins=50, density=True, alpha=0.7, 
                color='lightblue', edgecolor='black')
    x = np.linspace(-4, 4, 100)
    axes[0].plot(x, norm.pdf(x, 0, 1), 'r-', linewidth=2, label='N(0,1)')
    axes[0].set_title('æ ‡å‡†æ­£æ€åˆ†å¸ƒ Îµ ~ N(0,1)')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # ç›´æ¥é‡‡æ ·ç»“æœ
    axes[1].hist(samples_direct, bins=50, density=True, alpha=0.7, 
                color='lightgreen', edgecolor='black')
    x = np.linspace(mu-4*sigma, mu+4*sigma, 100)
    axes[1].plot(x, norm.pdf(x, mu, sigma), 'r-', linewidth=2, 
                label=f'N({mu},{sigma}Â²)')
    axes[1].set_title('ç›´æ¥é‡‡æ ·')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    # é‡å‚æ•°åŒ–ç»“æœ
    axes[2].hist(samples_reparam, bins=50, density=True, alpha=0.7, 
                color='lightcoral', edgecolor='black')
    axes[2].plot(x, norm.pdf(x, mu, sigma), 'r-', linewidth=2, 
                label=f'N({mu},{sigma}Â²)')
    axes[2].set_title('é‡å‚æ•°åŒ–: Î¼ + Ïƒ * Îµ')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    plt.suptitle('é‡å‚æ•°åŒ–æŠ€å·§æ¼”ç¤º', fontsize=14)
    plt.tight_layout()
    plt.show()
    
    # ç»Ÿè®¡éªŒè¯
    print(f"ç›´æ¥é‡‡æ · - å‡å€¼: {np.mean(samples_direct):.3f}, æ ‡å‡†å·®: {np.std(samples_direct):.3f}")
    print(f"é‡å‚æ•°åŒ– - å‡å€¼: {np.mean(samples_reparam):.3f}, æ ‡å‡†å·®: {np.std(samples_reparam):.3f}")
    print(f"ç†è®ºå€¼ - å‡å€¼: {mu}, æ ‡å‡†å·®: {sigma}")

demonstrate_reparameterization()
```

### æ‰©æ•£æ¨¡å‹ä¸­é«˜æ–¯å™ªå£°çš„æ ¸å¿ƒä¼˜åŠ¿

#### æ•°å­¦å…¬å¼æ¨å¯¼

$$
\begin{align}
\text{1. å‰å‘è¿‡ç¨‹å¯è§£æï¼š} \\
q(x_t|x_0) &= \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I) \\
\\
\text{2. é€†å‘è¿‡ç¨‹å¯æ¨å¯¼ï¼š} \\
q(x_{t-1}|x_t, x_0) &= \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I) \\
\text{å…¶ä¸­ï¼š} \quad \tilde{\mu}_t(x_t, x_0) &= \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})x_t + \sqrt{\bar{\alpha}_{t-1}}\beta_t x_0}{1-\bar{\alpha}_t} \\
\\
\text{3. å˜åˆ†ä¸‹ç•Œï¼š} \\
\mathcal{L} &= \mathbb{E}_q\left[\log\frac{q(x_{1:T}|x_0)}{p_\theta(x_{0:T})}\right] \\
&= \mathbb{E}_q\left[D_{KL}(q(x_T|x_0)||p(x_T)) + \sum_{t=2}^T D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t)) - \log p_\theta(x_0|x_1)\right]
\end{align}
$$

```python
def why_gaussian_noise():
    """è§£é‡Šä¸ºä»€ä¹ˆæ‰©æ•£æ¨¡å‹ä½¿ç”¨é«˜æ–¯å™ªå£°"""
    print("=" * 60)
    print("ä¸ºä»€ä¹ˆæ‰©æ•£æ¨¡å‹ä½¿ç”¨")
```

# ä¸ºä»€ä¹ˆæ‰©æ•£æ¨¡å‹çš„å‰å‘è¿‡ç¨‹è¦æ·»åŠ "é«˜æ–¯å™ªå£°"ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„é—®é¢˜ï¼è®©æˆ‘ä»å¤šä¸ªè§’åº¦æ¥è§£é‡Šä¸ºä»€ä¹ˆé€‰æ‹©é«˜æ–¯å™ªå£°è€Œä¸æ˜¯å…¶ä»–ç±»å‹çš„å™ªå£°ã€‚

## ğŸ¯ æ ¸å¿ƒåŸå› æ¦‚è¿°

é€‰æ‹©é«˜æ–¯å™ªå£°ä¸»è¦åŸºäºä»¥ä¸‹å‡ ä¸ªå…³é”®åŸå› ï¼š
1. **æ•°å­¦ä¸Šçš„ä¼˜é›…æ€§**
2. **ç‰©ç†ä¸–ç•Œçš„çœŸå®æ€§**
3. **è®¡ç®—ä¸Šçš„ä¾¿åˆ©æ€§**
4. **ç†è®ºä¸Šçš„æœ€ä¼˜æ€§**

---

## ğŸ“Š 1. æ•°å­¦ä¼˜é›…æ€§ï¼šä¸­å¿ƒæé™å®šç†

### ä¸­å¿ƒæé™å®šç†å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ

```mermaid
graph TD
    A[ä»»æ„åˆ†å¸ƒçš„éšæœºå˜é‡] --> B[å¤§é‡ç‹¬ç«‹éšæœºå˜é‡çš„å’Œ]
    B --> C[è¶‹å‘äºé«˜æ–¯åˆ†å¸ƒ]
    
    D[å›¾åƒåƒç´ å€¼] --> E[å—åˆ°å¤šç§éšæœºå› ç´ å½±å“]
    E --> F[å™ªå£°è‡ªç„¶å‘ˆé«˜æ–¯åˆ†å¸ƒ]
    
    style C fill:#e8f5e8
    style F fill:#e8f5e8
```

**ç®€å•ç†è§£ï¼š**
- ç°å®ä¸–ç•Œä¸­ï¼Œä»»ä½•æµ‹é‡éƒ½ä¼šå—åˆ°**æ— æ•°ä¸ªå¾®å°éšæœºå› ç´ **çš„å½±å“
- è¿™äº›å› ç´ å åŠ èµ·æ¥ï¼Œæœ€ç»ˆçš„å™ªå£°åˆ†å¸ƒä¼š**è‡ªç„¶åœ°è¶‹å‘é«˜æ–¯åˆ†å¸ƒ**
- æ¯”å¦‚ç›¸æœºæ‹ç…§æ—¶çš„çƒ­å™ªå£°ã€é‡å­å™ªå£°ã€ç”µè·¯å™ªå£°ç­‰

### æ•°å­¦è¡¨è¾¾
å¦‚æœæœ‰nä¸ªç‹¬ç«‹çš„éšæœºå˜é‡ $X_1, X_2, ..., X_n$ï¼Œå®ƒä»¬çš„å’Œï¼š

$$S_n = X_1 + X_2 + ... + X_n$$

å½“nè¶³å¤Ÿå¤§æ—¶ï¼Œ$S_n$ çš„åˆ†å¸ƒè¶‹å‘äºï¼š

$$S_n \sim \mathcal{N}(\mu, \sigma^2)$$

---

## ğŸŒ 2. ç‰©ç†çœŸå®æ€§ï¼šè‡ªç„¶ç•Œçš„å™ªå£°å°±æ˜¯é«˜æ–¯çš„

### çœŸå®ä¸–ç•Œçš„ä¾‹å­

```python
import numpy as np
import matplotlib.pyplot as plt

def demonstrate_natural_noise():
    """æ¼”ç¤ºè‡ªç„¶ç•Œä¸­çš„é«˜æ–¯å™ªå£°"""
    
    # æ¨¡æ‹Ÿç›¸æœºä¼ æ„Ÿå™¨å™ªå£°
    clean_signal = np.ones(1000) * 100  # ç†æƒ³ä¿¡å·å€¼
    
    # å„ç§å™ªå£°æº
    thermal_noise = np.random.normal(0, 2, 1000)      # çƒ­å™ªå£°
    shot_noise = np.random.normal(0, 1.5, 1000)       # æ•£ç²’å™ªå£°  
    read_noise = np.random.normal(0, 1, 1000)         # è¯»å–å™ªå£°
    
    # æ€»å™ªå£° = å„ç§å™ªå£°çš„å åŠ 
    total_noise = thermal_noise + shot_noise + read_noise
    noisy_signal = clean_signal + total_noise
    
    # å¯è§†åŒ–
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # å™ªå£°åˆ†å¸ƒ
    ax1.hist(total_noise, bins=50, alpha=0.7, density=True)
    ax1.set_title('çœŸå®ä¸–ç•Œå™ªå£°åˆ†å¸ƒ')
    ax1.set_xlabel('å™ªå£°å€¼')
    ax1.set_ylabel('æ¦‚ç‡å¯†åº¦')
    
    # æ‹Ÿåˆé«˜æ–¯åˆ†å¸ƒ
    mu, sigma = np.mean(total_noise), np.std(total_noise)
    x = np.linspace(-10, 10, 100)
    gaussian = (1/np.sqrt(2*np.pi*sigma**2)) * np.exp(-0.5*((x-mu)/sigma)**2)
    ax1.plot(x, gaussian, 'r-', linewidth=2, label='ç†è®ºé«˜æ–¯åˆ†å¸ƒ')
    ax1.legend()
    
    # ä¿¡å·å¯¹æ¯”
    ax2.plot(clean_signal[:100], 'b-', label='ç†æƒ³ä¿¡å·', linewidth=2)
    ax2.plot(noisy_signal[:100], 'r-', alpha=0.7, label='å«å™ªä¿¡å·')
    ax2.set_title('ä¿¡å· vs å™ªå£°ä¿¡å·')
    ax2.set_xlabel('æ ·æœ¬')
    ax2.set_ylabel('ä¿¡å·å€¼')
    ax2.legend()
    
    plt.tight_layout()
    plt.show()
    
    print(f"å™ªå£°å‡å€¼: {mu:.3f}")
    print(f"å™ªå£°æ ‡å‡†å·®: {sigma:.3f}")
    print("å¯ä»¥çœ‹åˆ°ï¼Œå¤šç§å™ªå£°å åŠ åç¡®å®å‘ˆç°é«˜æ–¯åˆ†å¸ƒï¼")

demonstrate_natural_noise()
```

---

## âš¡ 3. è®¡ç®—ä¾¿åˆ©æ€§ï¼šæ•°å­¦è¿ç®—ç®€å•

### é«˜æ–¯åˆ†å¸ƒçš„ä¼˜ç¾æ€§è´¨

#### æ€§è´¨1ï¼šçº¿æ€§ç»„åˆä»æ˜¯é«˜æ–¯åˆ†å¸ƒ
å¦‚æœ $X \sim \mathcal{N}(\mu_1, \sigma_1^2)$ï¼Œ$Y \sim \mathcal{N}(\mu_2, \sigma_2^2)$ï¼Œé‚£ä¹ˆï¼š

$$aX + bY \sim \mathcal{N}(a\mu_1 + b\mu_2, a^2\sigma_1^2 + b^2\sigma_2^2)$$

**è¿™æ„å‘³ç€ä»€ä¹ˆï¼Ÿ**
```python
def gaussian_linear_property():
    """æ¼”ç¤ºé«˜æ–¯åˆ†å¸ƒçš„çº¿æ€§æ€§è´¨"""
    
    # åŸå§‹æ•°æ®
    x0 = torch.randn(1000, 64, 64, 3)  # åŸå§‹å›¾åƒ
    
    # æ·»åŠ é«˜æ–¯å™ªå£°
    noise1 = torch.randn_like(x0) * 0.1
    noise2 = torch.randn_like(x0) * 0.2
    
    # çº¿æ€§ç»„åˆ
    x1 = 0.9 * x0 + noise1  # ä»ç„¶æ˜¯é«˜æ–¯åˆ†å¸ƒï¼
    x2 = 0.8 * x1 + noise2  # ä»ç„¶æ˜¯é«˜æ–¯åˆ†å¸ƒï¼
    
    print("æ— è®ºç»è¿‡å¤šå°‘æ­¥çº¿æ€§å˜æ¢ï¼Œåˆ†å¸ƒæ€§è´¨ä¿æŒä¸å˜ï¼")
    print("è¿™è®©æ•°å­¦æ¨å¯¼å˜å¾—éå¸¸ç®€å•")
```

#### æ€§è´¨2ï¼šé‡å‚æ•°åŒ–æŠ€å·§
$$x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$$

å…¶ä¸­ $\epsilon \sim \mathcal{N}(0, I)$ï¼Œè¿™ä¸ªå…¬å¼**åªå¯¹é«˜æ–¯åˆ†å¸ƒæˆç«‹**ï¼

### ä¸ºä»€ä¹ˆå…¶ä»–åˆ†å¸ƒä¸è¡Œï¼Ÿ

```python
def compare_noise_types():
    """æ¯”è¾ƒä¸åŒç±»å‹å™ªå£°çš„æ•ˆæœ"""
    
    x0 = torch.randn(100, 3, 32, 32)
    
    # é«˜æ–¯å™ªå£°
    gaussian_noise = torch.randn_like(x0)
    x_gaussian = 0.7 * x0 + 0.3 * gaussian_noise
    
    # å‡åŒ€å™ªå£°
    uniform_noise = torch.rand_like(x0) * 2 - 1  # [-1, 1]å‡åŒ€åˆ†å¸ƒ
    x_uniform = 0.7 * x0 + 0.3 * uniform_noise
    
    # æ‹‰æ™®æ‹‰æ–¯å™ªå£°
    laplace_noise = torch.distributions.Laplace(0, 1).sample(x0.shape)
    x_laplace = 0.7 * x0 + 0.3 * laplace_noise
    
    print("é—®é¢˜æ¥äº†ï¼š")
    print("1. å‡åŒ€åˆ†å¸ƒ + é«˜æ–¯åˆ†å¸ƒ â‰  å‡åŒ€åˆ†å¸ƒ")
    print("2. æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ + é«˜æ–¯åˆ†å¸ƒ â‰  æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ")
    print("3. åªæœ‰é«˜æ–¯åˆ†å¸ƒ + é«˜æ–¯åˆ†å¸ƒ = é«˜æ–¯åˆ†å¸ƒ")
    print("\nè¿™æ„å‘³ç€åªæœ‰é«˜æ–¯å™ªå£°èƒ½ä¿æŒåˆ†å¸ƒçš„ä¸€è‡´æ€§ï¼")
```

---

## ğŸ§® 4. ç†è®ºæœ€ä¼˜æ€§ï¼šæœ€å¤§ç†µåŸç†

### ä»€ä¹ˆæ˜¯æœ€å¤§ç†µåŸç†ï¼Ÿ

ç»™å®š**å‡å€¼**å’Œ**æ–¹å·®**çš„çº¦æŸä¸‹ï¼Œ**é«˜æ–¯åˆ†å¸ƒå…·æœ‰æœ€å¤§çš„ç†µ**ï¼ˆä¸ç¡®å®šæ€§ï¼‰ã€‚

$$H(X) = -\int p(x) \log p(x) dx$$

**ç›´è§‚ç†è§£ï¼š**
- ç†µè¶Šå¤§ = ä¸ç¡®å®šæ€§è¶Šå¤§ = ä¿¡æ¯é‡è¶Šå°‘
- åœ¨çº¦æŸæ¡ä»¶ä¸‹ï¼Œé«˜æ–¯åˆ†å¸ƒæ˜¯"æœ€éšæœº"çš„åˆ†å¸ƒ
- è¿™æ„å‘³ç€é«˜æ–¯å™ªå£°ä¸ä¼šå¼•å…¥ä»»ä½•**é¢å¤–çš„åè§**

```python
def entropy_comparison():
    """æ¯”è¾ƒä¸åŒåˆ†å¸ƒçš„ç†µ"""
    import scipy.stats as stats
    
    # ç›¸åŒæ–¹å·®çš„ä¸åŒåˆ†å¸ƒ
    variance = 1.0
    
    # é«˜æ–¯åˆ†å¸ƒçš„ç†µ
    gaussian_entropy = 0.5 * np.log(2 * np.pi * np.e * variance)
    
    # å‡åŒ€åˆ†å¸ƒçš„ç†µï¼ˆæ–¹å·®ç›¸åŒï¼‰
    # å¯¹äºå‡åŒ€åˆ†å¸ƒ U(a,b)ï¼Œæ–¹å·® = (b-a)Â²/12
    # æ‰€ä»¥ (b-a)Â²/12 = varianceï¼Œå¾—åˆ° b-a = 2âˆš(3*variance)
    width = 2 * np.sqrt(3 * variance)
    uniform_entropy = np.log(width)
    
    print(f"é«˜æ–¯åˆ†å¸ƒç†µ: {gaussian_entropy:.4f}")
    print(f"å‡åŒ€åˆ†å¸ƒç†µ: {uniform_entropy:.4f}")
    print(f"é«˜æ–¯åˆ†å¸ƒç†µæ›´å¤§ï¼Œæ„å‘³ç€æ›´'éšæœº'ï¼Œåè§æ›´å°‘")
```

---

## ğŸ”¬ 5. å®éªŒéªŒè¯ï¼šå…¶ä»–å™ªå£°çš„é—®é¢˜

### å®éªŒï¼šç”¨ä¸åŒå™ªå£°è®­ç»ƒæ‰©æ•£æ¨¡å‹

```python
class DiffusionWithDifferentNoise:
    def __init__(self, noise_type='gaussian'):
        self.noise_type = noise_type
    
    def add_noise(self, x0, t):
        """æ·»åŠ ä¸åŒç±»å‹çš„å™ªå£°"""
        if self.noise_type == 'gaussian':
            noise = torch.randn_like(x0)
        elif self.noise_type == 'uniform':
            noise = torch.rand_like(x0) * 2 - 1
        elif self.noise_type == 'laplace':
            noise = torch.distributions.Laplace(0, 1).sample(x0.shape)
        
        # æ‰©æ•£å…¬å¼ï¼ˆåªå¯¹é«˜æ–¯å™ªå£°ä¸¥æ ¼æˆç«‹ï¼‰
        alpha_bar_t = self.alphas_cumprod[t]
        x_t = torch.sqrt(alpha_bar_t) * x0 + torch.sqrt(1 - alpha_bar_t) * noise
        
        return x_t, noise

def compare_training_results():
    """æ¯”è¾ƒä¸åŒå™ªå£°ç±»å‹çš„è®­ç»ƒæ•ˆæœ"""
    
    results = {}
    noise_types = ['gaussian', 'uniform', 'laplace']
    
    for noise_type in noise_types:
        model = DiffusionWithDifferentNoise(noise_type)
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆç®€åŒ–ï¼‰
        losses = []
        for epoch in range(100):
            loss = train_epoch(model)  # å‡è®¾çš„è®­ç»ƒå‡½æ•°
            losses.append(loss)
        
        results[noise_type] = losses
    
    # å¯è§†åŒ–ç»“æœ
    plt.figure(figsize=(10, 6))
    for noise_type, losses in results.items():
        plt.plot(losses, label=f'{noise_type} å™ªå£°')
    
    plt.xlabel('è®­ç»ƒè½®æ¬¡')
    plt.ylabel('æŸå¤±')
    plt.title('ä¸åŒå™ªå£°ç±»å‹çš„è®­ç»ƒæ•ˆæœå¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.text(50, 0.8, 'é«˜æ–¯å™ªå£°æ”¶æ•›æœ€å¿«\nä¸”æœ€ç¨³å®šï¼', 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
    
    plt.show()
```

**å®éªŒç»“æœé€šå¸¸æ˜¾ç¤ºï¼š**
- âœ… **é«˜æ–¯å™ªå£°**ï¼šæ”¶æ•›å¿«ï¼Œç»“æœç¨³å®š
- âŒ **å‡åŒ€å™ªå£°**ï¼šæ”¶æ•›æ…¢ï¼Œå®¹æ˜“æŒ¯è¡
- âŒ **æ‹‰æ™®æ‹‰æ–¯å™ªå£°**ï¼šè®­ç»ƒä¸ç¨³å®šï¼Œç”Ÿæˆè´¨é‡å·®

---

## ğŸ¨ 6. ä»ç”Ÿæˆè´¨é‡è§’åº¦çœ‹

### é«˜æ–¯å™ªå£°ç”Ÿæˆçš„å›¾åƒæ›´è‡ªç„¶

```python
def visual_comparison():
    """å¯è§†åŒ–ä¸åŒå™ªå£°ç±»å‹çš„ç”Ÿæˆæ•ˆæœ"""
    
    # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
    def generate_with_noise(noise_type, steps=1000):
        # ä»çº¯å™ªå£°å¼€å§‹
        if noise_type == 'gaussian':
            x = torch.randn(1, 3, 64, 64)
        elif noise_type == 'uniform':
            x = torch.rand(1, 3, 64, 64) * 2 - 1
        elif noise_type == 'laplace':
            x = torch.distributions.Laplace(0, 1).sample((1, 3, 64, 64))
        
        # æ¨¡æ‹Ÿå»å™ªè¿‡ç¨‹
        for t in reversed(range(steps)):
            # è¿™é‡Œåªæ˜¯ç¤ºæ„ï¼Œå®é™…éœ€è¦è®­ç»ƒå¥½çš„æ¨¡å‹
            x = denoise_step(x, t)  # å‡è®¾çš„å»å™ªå‡½æ•°
        
        return x
    
    # ç”Ÿæˆå¯¹æ¯”å›¾
    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    
    noise_types = ['gaussian', 'uniform', 'laplace']
    titles = ['é«˜æ–¯å™ªå£°ç”Ÿæˆ', 'å‡åŒ€å™ªå£°ç”Ÿæˆ', 'æ‹‰æ™®æ‹‰æ–¯å™ªå£°ç”Ÿæˆ']
    
    for i, (noise_type, title) in enumerate(zip(noise_types, titles)):
        generated_image = generate_with_noise(noise_type)
        axes[i].imshow(tensor_to_image(generated_image))
        axes[i].set_title(title)
        axes[i].axis('off')
    
    plt.suptitle('ä¸åŒå™ªå£°ç±»å‹çš„ç”Ÿæˆæ•ˆæœå¯¹æ¯”')
    plt.tight_layout()
    plt.show()
```

---

## ğŸ“ˆ 7. ä¿¡æ¯è®ºè§’åº¦ï¼šæœ€å°åŒ–KLæ•£åº¦

### ä¸ºä»€ä¹ˆé«˜æ–¯åˆ†å¸ƒæ˜¯æœ€ä¼˜é€‰æ‹©ï¼Ÿ

ä»ä¿¡æ¯è®ºè§’åº¦ï¼Œæˆ‘ä»¬å¸Œæœ›**æœ€å°åŒ–çœŸå®åˆ†å¸ƒå’Œæ¨¡å‹åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦**ï¼š

$$D_{KL}(p_{data} \| p_{model}) = \int p_{data}(x) \log \frac{p_{data}(x)}{p_{model}(x)} dx$$

**å…³é”®æ´å¯Ÿï¼š**
- å¦‚æœçœŸå®æ•°æ®çš„å™ªå£°æœ¬èº«å°±æ˜¯é«˜æ–¯çš„
- é‚£ä¹ˆç”¨é«˜æ–¯å™ªå£°å»ºæ¨¡ä¼šå¾—åˆ°**æœ€å°çš„KLæ•£åº¦**
- è¿™æ„å‘³ç€æ¨¡å‹èƒ½æ›´å¥½åœ°æ‹ŸåˆçœŸå®æ•°æ®åˆ†å¸ƒ

```python
def kl_divergence_analysis():
    """åˆ†æä¸åŒå™ªå£°ç±»å‹çš„KLæ•£åº¦"""
    
    # å‡è®¾çœŸå®æ•°æ®å™ªå£°æ˜¯é«˜æ–¯çš„
    true_noise = torch.randn(10000)
    
    # ç”¨ä¸åŒåˆ†å¸ƒæ¥æ‹Ÿåˆ
    gaussian_model = torch.randn(10000)
    uniform_model = torch.rand(10000) * 2 - 1
    
    # è®¡ç®—KLæ•£åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    def compute_kl_approx(true_samples, model_samples):
        # ä½¿ç”¨ç›´æ–¹å›¾è¿‘ä¼¼è®¡ç®—KLæ•£åº¦
        bins = 50
        true_hist, _ = np.histogram(true_samples, bins=bins, density=True)
        model_hist, _ = np.histogram(model_samples, bins=bins, density=True)
        
        # é¿å…é™¤é›¶
        true_hist += 1e-8
        model_hist += 1e-8
        
        kl = np.sum(true_hist * np.log(true_hist / model_hist))
        return kl
    
    kl_gaussian = compute_kl_approx(true_noise, gaussian_model)
    kl_uniform = compute_kl_approx(true_noise, uniform_model)
    
    print(f"é«˜æ–¯æ¨¡å‹çš„KLæ•£åº¦: {kl_gaussian:.4f}")
    print(f"å‡åŒ€æ¨¡å‹çš„KLæ•£åº¦: {kl_uniform:.4f}")
    print("KLæ•£åº¦è¶Šå°è¶Šå¥½ï¼Œé«˜æ–¯å™ªå£°è·èƒœï¼")
```

---

## ğŸ¯ æ€»ç»“ï¼šä¸ºä»€ä¹ˆå¿…é¡»æ˜¯é«˜æ–¯å™ªå£°ï¼Ÿ

### æ ¸å¿ƒåŸå› æ€»ç»“

| è§’åº¦ | åŸå›  | é‡è¦æ€§ |
|------|------|--------|
| **æ•°å­¦** | ä¸­å¿ƒæé™å®šç†ï¼Œçº¿æ€§ç»„åˆå°é—­æ€§ | â­â­â­â­â­ |
| **ç‰©ç†** | è‡ªç„¶ç•Œå™ªå£°çš„çœŸå®åˆ†å¸ƒ | â­â­â­â­â­ |
| **è®¡ç®—** | é‡å‚æ•°åŒ–æŠ€å·§ï¼Œè®¡ç®—ç®€å• | â­â­â­â­ |
| **ç†è®º** | æœ€å¤§ç†µåŸç†ï¼Œæœ€å°åè§ | â­â­â­â­ |
| **å®éªŒ** | è®­ç»ƒç¨³å®šï¼Œç”Ÿæˆè´¨é‡é«˜ | â­â­â­â­â­ |
| **ä¿¡æ¯è®º** | æœ€å°åŒ–KLæ•£åº¦ | â­â­â­ |

### å…³é”®è¦ç‚¹

1. **ä¸æ˜¯äººä¸ºé€‰æ‹©**ï¼šé«˜æ–¯å™ªå£°æ˜¯åŸºäºæ·±åˆ»çš„æ•°å­¦å’Œç‰©ç†åŸç†
2. **ç†è®ºæ”¯æ’‘**ï¼šå¤šä¸ªç†è®ºéƒ½æŒ‡å‘é«˜æ–¯åˆ†å¸ƒæ˜¯æœ€ä¼˜é€‰æ‹©
3. **å®è·µéªŒè¯**ï¼šå®éªŒç»“æœè¯æ˜é«˜æ–¯å™ªå£°æ•ˆæœæœ€å¥½
4. **è®¡ç®—å‹å¥½**ï¼šä½¿å¾—å¤æ‚çš„æ•°å­¦æ¨å¯¼å˜å¾—ç®€å•

### å¦‚æœä¸ç”¨é«˜æ–¯å™ªå£°ä¼šæ€æ ·ï¼Ÿ

```python
def problems_without_gaussian():
    """ä¸ä½¿ç”¨é«˜æ–¯å™ªå£°çš„é—®é¢˜"""
    problems = {
        "æ•°å­¦é—®é¢˜": [
            "é‡å‚æ•°åŒ–æŠ€å·§å¤±æ•ˆ",
            "åˆ†å¸ƒæ€§è´¨ä¸ä¿æŒ",
            "ç†è®ºæ¨å¯¼å¤æ‚"
        ],
        "è®­ç»ƒé—®é¢˜": [
            "æ”¶æ•›é€Ÿåº¦æ…¢",
            "è®­ç»ƒä¸ç¨³å®š",
            "æ¢¯åº¦è®¡ç®—å›°éš¾"
        ],
        "ç”Ÿæˆé—®é¢˜": [
            "ç”Ÿæˆè´¨é‡å·®",
            "æ¨¡å¼å´©å¡Œ",
            "ç»†èŠ‚ä¸¢å¤±"
        ]
    }
    
    for category, issues in problems.items():
        print(f"\n{category}:")
        for issue in issues:
            print(f"  âŒ {issue}")
    
    print("\nâœ… ä½¿ç”¨é«˜æ–¯å™ªå£°é¿å…äº†æ‰€æœ‰è¿™äº›é—®é¢˜ï¼")

problems_without_gaussian()
```

**ç»“è®ºï¼šé«˜æ–¯å™ªå£°ä¸æ˜¯éšæ„é€‰æ‹©ï¼Œè€Œæ˜¯ç†è®ºã€å®è·µå’Œç›´è§‰çš„å®Œç¾ç»“åˆï¼** ğŸ¯

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå‡ ä¹æ‰€æœ‰æˆåŠŸçš„æ‰©æ•£æ¨¡å‹ï¼ˆDDPMã€DDIMã€Stable Diffusionç­‰ï¼‰éƒ½ä½¿ç”¨é«˜æ–¯å™ªå£°çš„æ ¹æœ¬åŸå› ã€‚