{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb5a551-11fb-4d04-b17b-fc99b6c90ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6db042e-103a-4dad-b0d2-5726724edd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, width, max_seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, width)\n",
    "\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(width):\n",
    "                if i % 2 == 0:\n",
    "                    pe[pos][i] = np.sin(pos/(10000 ** (i/width)))\n",
    "                else:\n",
    "                    pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/width)))\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bccbad7-f597-4651-8e6e-7694c94e8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, width, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        self.query = nn.Linear(width, head_size)\n",
    "        self.key = nn.Linear(width, head_size)\n",
    "        self.value = nn.Linear(width, head_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 计算K，Q，V\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Q和K的点积\n",
    "        attention = Q @ K.transpose(-2,-1)\n",
    "        # 缩放\n",
    "        attention = attention / (self.head_size ** 0.5)\n",
    "        # 掩码\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        attention = attention @ V\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67b8f8c-16ac-4513-93fc-8e7212c1a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, width, n_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = width // n_heads\n",
    "        self.W_o = nn.Linear(width, width)\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(width, self.head_size) for _ in range(n_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 拼接多个注意力头\n",
    "        out = torch.cat([head(x, mask=mask) for head in self.heads], dim=-1)\n",
    "        out = self.W_o(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e8b132-2903-4dfe-a12f-24df3eda0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, width, n_heads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 层归一化\n",
    "        self.ln1 = nn.LayerNorm(width)\n",
    "\n",
    "        # 多头注意力\n",
    "        self.mha = MultiHeadAttention(width, n_heads)\n",
    "\n",
    "        # 层归一化\n",
    "        self.ln2 = nn.LayerNorm(width)\n",
    "\n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.width, self.width*r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.width*r_mlp, self.width)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.mha(self.ln1(x), mask=mask)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e1d977-f61b-4c5b-aa78-2dd9e51682c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, encode=True, mask=None, max_seq_length=32):\n",
    "    if encode:\n",
    "        out = chr(2) + text + chr(3) # 添加 SOT token 和 EOT token\n",
    "        out = out + \"\".join([\n",
    "            chr(0) for _ in range(max_seq_length-len(out))\n",
    "        ]) # 添加Padding\n",
    "        out = torch.IntTensor(list(out.encode(\"utf-8\"))) # 对文本进行编码\n",
    "        mask = torch.ones(len(out.nonzero()))\n",
    "        mask = torch.cat((\n",
    "            mask,\n",
    "            torch.zeros(max_seq_length - len(mask))\n",
    "        )).type(torch.IntTensor)\n",
    "    else:\n",
    "        out = [chr(x) for x in text[1:len(mask.nonzero())-1]]\n",
    "        out = \"\".join(out)\n",
    "        mask = None\n",
    "\n",
    "    return out, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18974729-48ac-4fdd-aafe-47567d0d994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size, # 词汇表大小=256\n",
    "        width, # 宽度\n",
    "        max_seq_length, # 文本最大长度\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        emb_dim # 嵌入维度\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, width)\n",
    "        self.positional_embedding = PositionalEmbedding(width, max_seq_length)\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(width, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        # 可学习投影（projection）\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "    def forward(self, text, mask=None):\n",
    "        # 文本嵌入\n",
    "        x = self.encoder_embedding(text)\n",
    "        # 位置嵌入\n",
    "        x = self.positional_embedding(x)\n",
    "        # Transformer编码器\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x, mask=mask)\n",
    "        # 从EOT的嵌入抽取特征\n",
    "        x = x[torch.arange(text.shape[0]), torch.sub(torch.sum(mask[:,0],dim=1),1)]\n",
    "        # 将文本特征嵌入到联合嵌入空间中（多模态嵌入空间）\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc54cdba-125b-4503-b8b6-3d6eed823c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        n_channels,\n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        emb_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size[0] % patch_size[0] == 0  \\\n",
    "           and img_size[1] % patch_size[1] == 0, \\\n",
    "           \"img_size必须能被patch_size整除\"\n",
    "        assert width % n_heads == 0, \\\n",
    "           \"width必须能被n_heads整除\"\n",
    "\n",
    "        self.n_patches = (img_size[0] * img_size[1]) \\\n",
    "                      // (patch_size[0] * patch_size[1])\n",
    "        self.max_seq_length = self.n_patches + 1\n",
    "        self.linear_project = nn.Conv2d(\n",
    "            n_channels,\n",
    "            width,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, width))\n",
    "        self.positional_embedding = PositionalEmbedding(width,self.max_seq_length)\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(width,n_heads)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # 可学习的投影\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 补丁嵌入\n",
    "        x = self.linear_project(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # 位置嵌入\n",
    "        x = torch.cat((self.cls_token.expand(x.size()[0], -1, -1),x), dim=1)\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        # Transformer编码器\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x)\n",
    "\n",
    "        # 获取类别token\n",
    "        x = x[:, 0, :]\n",
    "\n",
    "        # 多模态嵌入\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2918a8bf-31c5-4761-8976-85554048dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        vit_width,\n",
    "        img_size,\n",
    "        patch_size,\n",
    "        n_channels,\n",
    "        vit_layers,\n",
    "        vit_heads,\n",
    "        vocab_size,\n",
    "        text_width,\n",
    "        max_seq_length,\n",
    "        text_heads,\n",
    "        text_layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(\n",
    "            vit_width,\n",
    "            img_size,\n",
    "            patch_size,\n",
    "            n_channels,\n",
    "            vit_layers,\n",
    "            vit_heads,\n",
    "            emb_dim\n",
    "        )\n",
    "        self.text_encoder = TextEncoder(\n",
    "            vocab_size,\n",
    "            text_width,\n",
    "            max_seq_length,\n",
    "            text_heads,\n",
    "            text_layers,\n",
    "            emb_dim\n",
    "        )\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "    def forward(self,image,text, mask=None):\n",
    "        I_e = self.image_encoder(image)\n",
    "        T_e = self.text_encoder(text, mask=mask)\n",
    "\n",
    "        # 缩放逐点余弦相似度[n, n]\n",
    "        logits = (I_e @ T_e.transpose(-2,-1)) * torch.exp(self.temperature)\n",
    "\n",
    "        # 对称损失函数\n",
    "        labels = torch.arange(logits.shape[0]).to(self.device)\n",
    "\n",
    "        loss_i = nn.functional.cross_entropy(logits.transpose(-2,-1), labels)\n",
    "        loss_t = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2aacd89-2730-4332-ade4-19a468939f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = load_dataset(\"./../datasets/clip-mnist/\")\n",
    "        self.transform = T.ToTensor()\n",
    "        if train:\n",
    "            self.split = \"train\"\n",
    "        else:\n",
    "            self.split = \"test\"\n",
    "        \n",
    "        self.captions = {\n",
    "            0: \"An image of 0\",\n",
    "            1: \"An image of 1\",\n",
    "            2: \"An image of 2\",\n",
    "            3: \"An image of 3\",\n",
    "            4: \"An image of 4\",\n",
    "            5: \"An image of 5\",\n",
    "            6: \"An image of 6\",\n",
    "            7: \"An image of 7\",\n",
    "            8: \"An image of 8\",\n",
    "            9: \"An image of 9\"\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows[self.split]\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        img = self.dataset[self.split][i][\"image\"]\n",
    "        img = self.transform(img)\n",
    "        cap, mask = tokenizer(self.captions[self.dataset[self.split][i][\"label\"]])\n",
    "        mask = mask.repeat(len(mask), 1)\n",
    "        return {\"image\": img, \"caption\": cap, \"mask\": mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33d120f9-cb7f-4700-a3b9-6785f9ac2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 32\n",
    "vit_width = 9\n",
    "img_size = (28,28)\n",
    "patch_size = (14,14)\n",
    "n_channels = 1\n",
    "vit_layers = 3\n",
    "vit_heads = 3\n",
    "vocab_size = 256\n",
    "text_width = 32\n",
    "max_seq_length = 32\n",
    "text_heads = 8\n",
    "text_layers = 4\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d0cbb3-3174-4a12-9d86-f8a4cbedde73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c5e16c2346467ea4a86909dc9e5bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c947acd0e3c84e77810680c743564558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set = MNIST(train = True)\n",
    "test_set = MNIST(train = False)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d94a8e8-2554-48a8-961b-ed9b365176f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch Loss: 2.872\n",
      "模型已经保存.\n",
      "Epoch [2/10], Batch Loss: 2.730\n",
      "模型已经保存.\n",
      "Epoch [3/10], Batch Loss: 2.629\n",
      "模型已经保存.\n",
      "Epoch [4/10], Batch Loss: 2.564\n",
      "模型已经保存.\n",
      "Epoch [5/10], Batch Loss: 2.583\n",
      "Epoch [6/10], Batch Loss: 2.648\n",
      "Epoch [7/10], Batch Loss: 2.459\n",
      "模型已经保存.\n",
      "Epoch [8/10], Batch Loss: 2.522\n",
      "Epoch [9/10], Batch Loss: 2.520\n",
      "Epoch [10/10], Batch Loss: 2.470\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = CLIP(\n",
    "    emb_dim,\n",
    "    vit_width,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    vit_layers,\n",
    "    vit_heads,\n",
    "    vocab_size,\n",
    "    text_width,\n",
    "    max_seq_length,\n",
    "    text_heads,\n",
    "    text_layers\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        img = data[\"image\"].to(device)\n",
    "        cap = data[\"caption\"].to(device)\n",
    "        mask = data[\"mask\"].to(device)\n",
    "        loss = model(img, cap, mask)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Batch Loss: {loss.item():.3f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    if loss.item() <= best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), \"./clip.pt\")\n",
    "        print(\"模型已经保存.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf3fd966-faee-4f1b-9e2a-2a957a486d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "预测准确率: 94 %\n"
     ]
    }
   ],
   "source": [
    "# 加载最好的模型\n",
    "model = CLIP(\n",
    "    emb_dim,\n",
    "    vit_width,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    vit_layers,\n",
    "    vit_heads,\n",
    "    vocab_size,\n",
    "    text_width,\n",
    "    max_seq_length,\n",
    "    text_heads,\n",
    "    text_layers\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"./clip.pt\", map_location=device))\n",
    "\n",
    "# 获取数据集的标签和图片进行对比\n",
    "text = torch.stack([tokenizer(x)[0] for x in test_set.captions.values()]).to(device)\n",
    "mask = torch.stack([tokenizer(x)[1] for x in test_set.captions.values()])\n",
    "mask = mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)\n",
    "\n",
    "correct, total = 0,0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data[\"image\"].to(device), data[\"caption\"].to(device)\n",
    "        image_features = model.image_encoder(images)\n",
    "        text_features = model.text_encoder(text, mask=mask)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        _, indices = torch.max(similarity,1)\n",
    "        pred = torch.stack([tokenizer(test_set.captions[int(i)])[0] for i in indices]).to(device)\n",
    "        correct += int(sum(torch.sum((pred==labels),dim=1)//len(pred[0])))\n",
    "        total += len(labels)\n",
    "\n",
    "print(f'\\n预测准确率: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fa24f49-789e-495a-ad8a-c11ba4cd4996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "预测结果:\n",
      "\n",
      "    a image of 9: 100.00%\n",
      "     a 9 of nine: 0.00%\n",
      "            asdf: 0.00%\n",
      "            fdsa: 0.00%\n",
      "               c: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = CLIP(\n",
    "    emb_dim,\n",
    "    vit_width,\n",
    "    img_size,\n",
    "    patch_size,\n",
    "    n_channels,\n",
    "    vit_layers,\n",
    "    vit_heads,\n",
    "    vocab_size,\n",
    "    text_width,\n",
    "    max_seq_length,\n",
    "    text_heads,\n",
    "    text_layers\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"./clip.pt\", map_location=device))\n",
    "\n",
    "# 标题\n",
    "class_names = [\"a\", \"df\", \"c\", \"asdf\", \"fdsa\", \"xxx\", \"A Image of xxx\", \"xy\", \"yx\", \"a 9 of nine\", \"a image of 9\"]\n",
    "\n",
    "text = torch.stack([tokenizer(x)[0] for x in class_names]).to(device)\n",
    "mask = torch.stack([tokenizer(x)[1] for x in class_names])\n",
    "mask = mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)\n",
    "\n",
    "idx = 1000\n",
    "\n",
    "img = test_set[idx][\"image\"][None,:]\n",
    "plt.imshow(img[0].permute(1, 2, 0)  ,cmap=\"gray\")\n",
    "plt.title(tokenizer(test_set[idx][\"caption\"], encode=False, mask=test_set[idx][\"mask\"][0])[0])\n",
    "plt.show()\n",
    "img = img.to(device)\n",
    "with torch.no_grad():\n",
    "    image_features = model.image_encoder(img)\n",
    "    text_features = model.text_encoder(text, mask=mask)\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# 打印结果\n",
    "print(\"\\n预测结果:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{class_names[int(index)]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765b103-6361-4eb4-8a64-8f9df4a675ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
