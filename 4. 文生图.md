
去噪扩散概率模型 (DDPM) 是一种流行的生成式人工智能模型，由 Ho 等人于 2020 年提出，并由 Nichol 等人于 2021 年对其进行了改进。这些模型背后的基本思想是，在正向扩散过程中将噪声添加到图像中，以便训练模型预测在反向扩散过程中应在特定时间步长去除的噪声。在对图像进行采样时，需要从包含纯噪声的图像开始，并在每个时间步迭代地去除模型预测的噪声，直到获得最终图像。

为了让 DDPM 生成多种类型的图像，同时仍允许用户选择所需的图像类型，模型需要根据某些输入进行条件调节。Ramesh 等人提出了一种名为 unCLIP 的条件调节方法，该方法已用于 OpenAI 的 DALL-E 2 模型。在 Ramesh 等人描述的方法中，输入的标题首先被传递到一个先验网络（prior network），该网络将使用经过训练的 CLIP 模型来获取 CLIP 文本嵌入。然后，仅解码器的 Transformer 使用这些文本嵌入来生成可能的 CLIP 图像嵌入。先验网络生成的 CLIP 图像嵌入将被解码器网络（由 UNet 模型组成）用于条件调节所创建的图像。在本文中，我们将使用此过程构建一个简单的扩散模型。

![[Pasted image 20250906134643.png]]

## CLIP

为了从文本创建扩散图像，我们将使用 CLIP 模型中的嵌入。从 CLIP 获得的文本嵌入用于调节先验模型，使其扩散相应的图像嵌入。然后，这些图像嵌入用于调节解码器模型，帮助其引导至所需的图像。

可以参考第二章： [[2. CLIP]]

## 正向扩散

![[Pasted image 20250906135110.png]]

前向扩散是扩散过程的一部分，其中在 $T$ 步的每个时间步内将高斯噪声添加到图像中。每个时间步应施加的噪声量由方差表给出。前向扩散的公式为：

![[Pasted image 20250906135233.png]]

也就是说，我们在每个时间步上持续添加高斯噪声。虽然上面的公式迭代地添加噪声，但通过使用重参数化技巧，我们能够以闭式形式（解析解）在任意时间步直接采样带噪声的图像。我们将使用此方法进行前向扩散过程。该方法的公式为：

![[Pasted image 20250906135316.png]]

对于方差调度，在 Ho 等人的原始 DDPM 论文中，采用了线性调度。虽然该调度在高分辨率图像上效果良好，但当图像较小（64x64 或更小）时，正向扩散过程最终会产生过多的噪声。为了解决这个问题， Nichol 等人建议使用余弦调度，其公式如下所示。

![[Pasted image 20250906135403.png]]

这些值被剪裁为小于或等于 0.999，以防止在 t = T 附近出现奇点。因为我们使用低分辨率图像，所以我们将使用余弦 beta 计划。

综合起来，前向扩散过程的代码看起来是这样的：

```python
# Gets elements from indicies and makes sure output has a certain dimension
def extract_and_expand(x, idx, shape):
    return x[idx].reshape(idx.shape[0], *((1, ) * (len(shape) - 1)))

# Returns beta schedule
def get_beta_schedule(schedule, max_time, s=0.008):
    if schedule == "linear":
        scale = 1000 / max_time
        betas = torch.linspace(1e-4  * scale, 0.02  * scale, max_time)
    elif schedule == "cosine":
        t = torch.linspace(0, max_time, max_time + 1)
        a_bars = torch.cos((((t / max_time) + s) / (1 + s)) * (np.pi / 2)) ** 2
        a_bars = a_bars / a_bars[0]
        betas = 1 - (a_bars[1:] / a_bars[:-1])
        betas = torch.clamp(betas, min=0, max=0.999)
    else:
        Exception("Beta schedule not implemented.")

    return betas

def get_schedule_values(config):
        schedule_values = {}
        schedule_values["betas"] = get_beta_schedule(config.prior.schedule, config.prior.max_time).to(config.device)
        schedule_values["alphas"] = 1.0 - schedule_values["betas"]
        schedule_values["alpha_bars"] = torch.cumprod(schedule_values["alphas"], axis = 0)
        schedule_values["sqrt_recip_alphas"] = torch.sqrt(1.0 / schedule_values["alphas"])
        schedule_values["sqrt_alpha_bars"] = torch.sqrt(schedule_values["alpha_bars"])
        schedule_values["sqrt_one_minus_alpha_bars"] = torch.sqrt(1.0 - schedule_values["alpha_bars"])
        schedule_values["alpha_bars_prev"] = torch.cat((torch.ones(1, device=config.device), schedule_values["alpha_bars"][:-1]))
        schedule_values["sigma"] = schedule_values["betas"] * (1.0 - schedule_values["alpha_bars_prev"]) / (1.0 - schedule_values["alpha_bars"])
        return schedule_values

# Gets noisy image at a certain timestep
def forward_diffusion(x_0, schedule_values, t):
    noise = torch.randn_like(x_0)
    sqrt_alpha_bars = extract_and_expand(schedule_values["sqrt_alpha_bars"], t, x_0.shape)
    sqrt_one_minus_alpha_bars = extract_and_expand(schedule_values["sqrt_one_minus_alpha_bars"], t, x_0.shape)
    x_noisy = (sqrt_alpha_bars * x_0) + (sqrt_one_minus_alpha_bars * noise)
    return x_noisy, noise
```

## 时间步嵌入

时间步嵌入是扩散的重要组成部分。这是因为不同时间步的图像具有不同的噪声量。为了在我们的模型中利用这些信息，我们将使用正弦位置编码。这些位置编码与 Transformer 中常用的位置编码相同。主要区别在于，我们的输入时间步很可能不会按顺序排列，并且包含所有可能的时间步，因此我们只需要获取与输入时间步对应的位置编码。

```python
class SinusoidalPositionalEncodings(nn.Module):
    def __init__(self, 
                 max_seq_length,    # Maximum sequence length
                 width              # Width of model
            ):
        super().__init__()

        # Create positional encodings
        pe = torch.zeros(max_seq_length, width)

        for pos in range(max_seq_length):
            for i in range(width):
                if i % 2 == 0:
                    pe[pos][i] = np.sin(pos / (10000 ** (i / width)))
                else:
                    pe[pos][i] = np.cos(pos / (10000 ** ((i - 1) / width)))

        self.register_buffer('pe', pe)
        
    def forward(self, x):
        # Get positional encodings corresponding to inputted timesteps
        x = self.pe[x]

        return x
```

然后，这些时间步编码通过 MLP 来进一步捕获时间信息。

```python
self.time_mlp = nn.Sequential(
    SinusoidalPositionalEmbedding(config.decoder.max_time, config.decoder.model_channels),
    nn.Linear(config.decoder.model_channels, config.decoder.cond_channels),
    nn.SiLU(),
    nn.Linear(config.decoder.cond_channels, config.decoder.cond_channels)
)
```

有多种方法可以调节此时间步嵌入的输入，这些方法在本文的 UNet：残差块部分中进行了描述。

## 先验模型（Prior Model）

![[Pasted image 20250906135725.png]]

先验模型用于从文本标题生成 CLIP 图像嵌入。也可以放弃先验模型，而以 CLIP 文本嵌入为条件，而不是先验模型生成的 CLIP 图像嵌入，但使用先验模型的效果最佳。

先验模型主要有两种类型：自回归先验和扩散先验。对于我们的模型，我们将使用扩散先验，因为 Ramesh 等人（2022 年）发现，在模型规模相当的情况下，扩散先验的表现优于自回归先验，并且减少了训练计算量。

扩散先验首先将文本标题作为输入，然后获取 CLIP 文本和图像嵌入。加载 CLIP 模型时，所有层都应冻结，并将模式设置为 eval。

```python
def freeze_model(model, set_eval=True):
    if set_eval:
        model.eval()

    for param in model.parameters():
        param.requires_grad = False

# Constructor
self.clip = CLIP(config).to(config.device)
self.clip.load_state_dict(torch.load(config.clip.model_location, map_location=config.device))
freeze_model(self.clip)

# Forward
image_embeddings = self.clip.image_encoder(images) # (B, C, H, W) -> (B, latent_dim)
text_embeddings = self.clip.text_encoder(captions, mask=masks) # (B, text_seq_length) -> (B, latent_dim)
```

然后，它会获取批次中每个图像的随机时间步，并使用它们从正向扩散中获取噪声 CLIP 图像嵌入。

```python
# Forward
timesteps = torch.randint(0, self.config.prior.max_time, (images.shape[0],)) # (B, )

noisy_image_embedding, _ = forward_diffusion(image_embeddings, self.schedule_values, timesteps)
```

然后将时间步输入到 MLP 以获得时间步长嵌入。

```python
# Constructor
self.time_mlp = nn.Sequential(
    SinusoidalPositionalEmbedding(config.prior.max_time, config.latent_dim),
    nn.Linear(config.latent_dim, config.latent_dim * config.prior.r_mlp, bias=config.prior.bias),
    nn.SiLU(),
    nn.Linear(config.latent_dim * config.prior.r_mlp, config.latent_dim, bias=config.prior.bias)
)

# Forward
timestep_embeddings = self.time_mlp(timesteps) # (B, ) -> (B, latent_dim)
```

先验模型的另一个重要部分是学习到的嵌入。这些嵌入是 Torch 的一个参数，将用于预测最终输出。从构造函数中学习到的嵌入需要在 Forward 方法中进行扩展，以便批次中的每个条目都有一个嵌入。

```python
# Constructor
self.learned_embedding = nn.Parameter(torch.randn(config.latent_dim))

# Forward
learned_embeddings = self.learned_embedding.repeat(images.shape[0], 1) # (latent_dim) -> (B, latent_dim)
```

文本标题、CLIP 文本嵌入、时间步嵌入、噪声 CLIP 图像嵌入以及学习到的嵌入将被连接成一个序列。所有这些项都将具有形状 `(B, latent_dim)` ，但我们需要在中间添加一个额外的维度，使它们具有形状 `(B, 1, latent_dim)` 。我们将在这个新维度上进行连接，使序列具有形状 `(B, 5, latent_dim)` 。如果使用包含更高质量图像的数据集，一种提高模型质量的可能方法是将文本嵌入、图像嵌入和/或时间步嵌入传递到卷积层，以增加序列长度。

```python
tokens = torch.cat((
    captions,               # Image Caption
    text_embeddings,        # CLIP Text Embedding
    timestep_embeddings,    # Timestep Embedding
    noisy_image_embedding,  # Noisy CLIP Image Embedding
    learned_embeddings      # Learned Embedding
), dim=1) # (B, 5, latent_dim)
```

然后，该序列会通过一个带有因果注意力掩码的仅解码器的 Transformer。因果注意力掩码是一个由 1 组成的下三角矩阵，它使得 token 只能关注在它之前出现的 token。

```python
# Constructor
self.decoder = nn.ModuleList(
    [TransformerBlock(
        config.latent_dim,
        cond_width=config.latent_dim,
        n_heads=config.prior.n_heads,
        dropout=config.prior.dropout,
        r_mlp=config.prior.r_mlp,
        bias=config.prior.bias
    ) for _ in range(config.prior.n_layers)]
)

self.register_buffer("causal_attention_mask", torch.tril(torch.ones(5, 5))[None, :])

# Forward
for block in self.decoder:
    tokens = block(tokens, mask=self.causal_attention_mask)
```

最后，我们从 Transformer 的输出中获取学习到的嵌入，并将其传递给 LayerNorm 和 Linear 层以获得预测的图像嵌入。

```python
# Constructor
self.output = nn.Sequential(
    nn.LayerNorm(config.latent_dim),
    nn.Linear(config.latent_dim, config.latent_dim, bias=config.decoder.bias)
)

# Forward
pred_image_embeddings = self.output(tokens[:, -1, :])
```

虽然通常情况下，扩散模型会预测噪声，并通过迭代去除噪声来生成样本，但 unCLIP 论文中提到，最好直接预测 CLIP 图像嵌入。该模型的损失函数应该是预测值与实际 CLIP 图像嵌入之间的均方误差损失。

```python
loss = nn.functional.mse_loss(pred_image_embeddings, image_embeddings)
```

在采样过程中，为了提高质量，模型应该生成两个 CLIP 图像嵌入的样本，并选择与 CLIP 文本嵌入点积较高的样本。

```python
def get_one_sample(self, text_embeddings, captions):
    # Get image embeddings that are pure noise
    noisy_image_embeddings = torch.randn(text_embeddings.shape, device=self.config.device)

    # timestep is max for all items because image embeddings are pure noise
    timesteps = torch.full((captions.shape[0],), self.config.prior.max_time - 1)

    # Get timestep embeddings
    timestep_embeddings = self.time_mlp(timesteps) # (B, ) -> (B, latent_dim)
    timestep_embeddings = timestep_embeddings[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

    # Expand learned embedding so that there is one for each item in batch
    learned_embeddings = self.learned_embedding.repeat(captions.shape[0], 1) # (latent_dim) -> (B, latent_dim)
    learned_embeddings = learned_embeddings[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

    tokens = torch.cat((
        captions,               # Image Caption
        text_embeddings,        # CLIP Text Embedding
        timestep_embeddings,    # Timestep Embedding
        noisy_image_embeddings,  # Noisy CLIP Image Embedding
        learned_embeddings      # Learned Embedding
    ), dim=1) # (B, 5, latent_dim)

    # Pass through transformer blocks with causal attention mask
    for block in self.decoder:
        tokens = block(tokens, mask=self.causal_attention_mask)

    # Get learned embeddings and pass through output projection to get CLIP image embeddings
    pred_image_embeddings = self.output(tokens[:, -1, :])

    return pred_image_embeddings

def sample(self, captions, masks=None):
    # Get CLIP text embeddings
    t_emb = self.clip.text_encoder(captions, mask=masks) # (B, text_seq_length) -> (B, latent_dim)
    text_embeddings = t_emb[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

    # Make caption length equal to latent dimension
    if self.config.text_seq_length >= self.config.latent_dim:
        captions = captions[:, :self.config.latent_dim]  # (B, max_seq_len) -> (B, latent_dim)
    else:
        captions = nn.functional.pad(captions, (0, self.config.latent_dim - self.config.text_seq_length)) # (B, max_seq_len) -> (B, latent_dim)

    captions = captions[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

    # Getting two samples
    sample_1 = self.get_one_sample(text_embeddings, captions)
    sample_2 = self.get_one_sample(text_embeddings, captions)

    gen_image_embeddings = torch.zeros(sample_1.shape)

    # Choosing the samples with the higher dot product with text embeddings
    for i in range(gen_image_embeddings.shape[0]):
        if sample_1[i] @ t_emb[i] >= sample_2[i] @ t_emb[i]:
            gen_image_embeddings[i] = sample_1[i]
        else:
            gen_image_embeddings[i] = sample_2[i]

    return gen_image_embeddings
```

综合以上所有，扩散先验模型的代码看起来应该是这样的：

```python
class DiffusionPrior(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Loading CLIP Model
        self.clip = CLIP(config).to(config.device)
        self.clip.load_state_dict(torch.load(config.clip.model_location, map_location=config.device))
        freeze_model(self.clip)

        self.config = config

        self.time_mlp = nn.Sequential(
            SinusoidalPositionalEmbedding(config.prior.max_time, config.latent_dim),
            nn.Linear(config.latent_dim, config.latent_dim * config.prior.r_mlp, bias=config.prior.bias),
            nn.SiLU(),
            nn.Linear(config.latent_dim * config.prior.r_mlp, config.latent_dim, bias=config.prior.bias)
        )

        self.learned_embedding = nn.Parameter(torch.randn(config.latent_dim))

        self.schedule_values = get_schedule_values(config)

        # Transformer blocks
        self.decoder = nn.ModuleList(
            [TransformerBlock(
                config.latent_dim,
                cond_width=config.latent_dim,
                n_heads=config.prior.n_heads,
                dropout=config.prior.dropout,
                r_mlp=config.prior.r_mlp,
                bias=config.prior.bias
            ) for _ in range(config.prior.n_layers)]
        )

        # Output Projection
        self.output = nn.Sequential(
            nn.LayerNorm(config.latent_dim),
            nn.Linear(config.latent_dim, config.latent_dim, bias=config.decoder.bias)
        )

        self.register_buffer("causal_attention_mask", torch.tril(torch.ones(5, 5))[None, :])

    def get_one_sample(self, text_embeddings, captions):
        # Get image embeddings that are pure noise
        noisy_image_embeddings = torch.randn(text_embeddings.shape, device=self.config.device)

        # timestep is max for all items because image embeddings are pure noise
        timesteps = torch.full((captions.shape[0],), self.config.prior.max_time - 1)

        # Get timestep embeddings
        timestep_embeddings = self.time_mlp(timesteps) # (B, ) -> (B, latent_dim)
        timestep_embeddings = timestep_embeddings[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

        # Expand learned embedding so that there is one for each item in batch
        learned_embeddings = self.learned_embedding.repeat(captions.shape[0], 1) # (latent_dim) -> (B, latent_dim)
        learned_embeddings = learned_embeddings[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

        tokens = torch.cat((
            captions,               # Image Caption
            text_embeddings,        # CLIP Text Embedding
            timestep_embeddings,    # Timestep Embedding
            noisy_image_embeddings,  # Noisy CLIP Image Embedding
            learned_embeddings      # Learned Embedding
        ), dim=1) # (B, 5, latent_dim)

        # Pass through transformer blocks with causal attention mask
        for block in self.decoder:
            tokens = block(tokens, mask=self.causal_attention_mask)

        # Get learned embeddings and pass through output projection to get CLIP image embeddings
        pred_image_embeddings = self.output(tokens[:, -1, :])

        return pred_image_embeddings

    def sample(self, captions, masks=None):
        # Get CLIP text embeddings
        t_emb = self.clip.text_encoder(captions, mask=masks) # (B, text_seq_length) -> (B, latent_dim)
        text_embeddings = t_emb[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

        # Make caption length equal to latent dimension
        if self.config.text_seq_length >= self.config.latent_dim:
            captions = captions[:, :self.config.latent_dim]  # (B, max_seq_len) -> (B, latent_dim)
        else:
            captions = nn.functional.pad(captions, (0, self.config.latent_dim - self.config.text_seq_length)) # (B, max_seq_len) -> (B, latent_dim)

        captions = captions[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

        # Getting two samples
        sample_1 = self.get_one_sample(text_embeddings, captions)
        sample_2 = self.get_one_sample(text_embeddings, captions)

        gen_image_embeddings = torch.zeros(sample_1.shape)

        # Choosing the samples with the higher dot product with text embeddings
        for i in range(gen_image_embeddings.shape[0]):
            if sample_1[i] @ t_emb[i] >= sample_2[i] @ t_emb[i]:
                gen_image_embeddings[i] = sample_1[i]
            else:
                gen_image_embeddings[i] = sample_2[i]

        return gen_image_embeddings

    def forward(self, images, captions, masks=None):
        # Get CLIP image embeddings
        image_embeddings = self.clip.image_encoder(images) # (B, C, H, W) -> (B, latent_dim)

        # Get CLIP text embeddings
        text_embeddings = self.clip.text_encoder(captions, mask=masks) # (B, text_seq_length) -> (B, latent_dim)
        text_embeddings = text_embeddings[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

        # Make caption length equal to latent dimension
        if self.config.text_seq_length >= self.config.latent_dim:
            captions = captions[:, :self.config.latent_dim]  # (B, max_seq_len) -> (B, latent_dim)
        else:
            captions = nn.functional.pad(captions, (0, self.config.latent_dim - self.config.text_seq_length)) # (B, max_seq_len) -> (B, latent_dim)

        captions = captions[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

        # Get random timesteps for forward diffusion
        timesteps = torch.randint(0, self.config.prior.max_time, (images.shape[0],)) # (B, )

        # Get timestep embeddings
        timestep_embeddings = self.time_mlp(timesteps) # (B, ) -> (B, latent_dim)
        timestep_embeddings = timestep_embeddings[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

        # Perform forward diffusion to get noisy CLIP image embeddings
        noisy_image_embedding, _ = forward_diffusion(image_embeddings, self.schedule_values, timesteps)
        noisy_image_embedding = noisy_image_embedding[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

        # Expand learned embedding so that there is one for each item in batch
        learned_embeddings = self.learned_embedding.repeat(images.shape[0], 1) # (latent_dim) -> (B, latent_dim)
        learned_embeddings = learned_embeddings[:, None, :] # (B, latent_dim) -> (B, 1, latent_dim)

        tokens = torch.cat((
            captions,               # Image Caption
            text_embeddings,        # CLIP Text Embedding
            timestep_embeddings,    # Timestep Embedding
            noisy_image_embedding,  # Noisy CLIP Image Embedding
            learned_embeddings      # Learned Embedding
        ), dim=1) # (B, 5, latent_dim)

        # Pass through transformer blocks with causal attention mask
        for block in self.decoder:
            tokens = block(tokens, mask=self.causal_attention_mask)

        # Get learned embeddings and pass through output projection to get CLIP image embeddings
        pred_image_embeddings = self.output(tokens[:, -1, :])

        loss = nn.functional.mse_loss(pred_image_embeddings, image_embeddings)

        return loss
```

## 扩散解码器（diffusion decoder）概述

![[Pasted image 20250906141222.png]]

扩散解码器是模型中创建图像的部分。它通过预测每个时间步应去除的噪声，并迭代地从噪声图像中去除预测的噪声来实现这一点。在 Ramesh 等人使用的 unCLIP 模型中，他们使用的模型与 Nichol 等人的 GLIDE 模型极为相似，我们也将密切关注该模型。

最常用作其主干的两个模型架构是 UNet 和 Transformers。

Transformers 方法的主要优势之一是其可扩展性，它可以很好地扩展至大型数据集和更复杂的模型。另一个优势是，虽然 UNet 主要用于图像，但 Transformers 更加灵活，只需进行显著修改即可用于其他数据类型。

UNet 是许多图像相关任务的主要选择。这是因为它擅长通过卷积层获取局部信息，同时通过跳跃连接保持高分辨率特征。UNet 的另一个优点是输入和输出的形状应该相同，这在图像扩散中非常有用。

UNet 的工作原理是，接收输入并将其传入编码器层，编码器层会识别/捕获相关特征，同时降低分辨率。然后，输入会传入解码器层，解码器层会尝试定位特征，同时将分辨率提升至原始形状。由于编码器层包含空间信息，因此会从编码器到解码器添加跳跃连接，以帮助保留这些信息。

## Decoder: Residual Blocks  解码器：残差块

![[Pasted image 20250906141410.png]]

编码器和解码器层均由残差块构成，这些残差块包含用于识别和定位特征的卷积。这些残差块通常由归一化、激活、卷积、归一化、激活、卷积组成；然而，我们的残差块将是归一化、卷积、归一化、激活、卷积、归一化（如图所示）。Han 等人的论文证明了这种架构可以在保持非线性的同时提升性能。

```python
# Constructor
self.layers1 = nn.Sequential(
    nn.GroupNorm(n_groups, d_in),
    nn.Conv2d(d_in, d_out, kernel_size, padding=1)
)

self.layers2 = nn.Sequential(
    nn.GroupNorm(n_groups, d_out),
    nn.SiLU(),
    nn.Dropout(p=dropout),
    nn.Conv2d(d_out, d_out, kernel_size, padding=1),
    nn.GroupNorm(n_groups, d_out)
)

# Forward
x = self.layers1(x_0)

x = self.layers2(x)
```

对于残差块的归一化部分，我们将使用 GroupNorm。之所以使用 GroupNorm 而不是 BatchNorm，是因为它的性能与批次大小无关，这使得它在小批次或可变批次大小上表现更佳。使用 GroupNorm 也有助于提高训练期间的稳定性。

残差块的激活部分用于实现非线性。虽然 ReLU 通常用于残差网络，但我们将使用 SiLU 作为激活函数。在 Ramachandran 等人的论文中，尽管模型和超参数是专门为 ReLU 设置的，但 SiLU 的表现优于 ReLU 和其他激活函数。由于 SiLU 的简单性以及与 ReLU 的相似性，因此只需用它来代替 ReLU 即可轻松实现。

对于卷积，我们将使用 3x3 核大小和 SAME 填充的 Conv2d 来保留空间维度。SAME 填充的工作原理是在输入的边界添加零，以确保输出形状与未填充时的输入形状相同。第一个卷积会将输入从 `d_in` 通道投影到 `d_out` 通道，而第二个卷积则仅保留 `d_out` 通道。

残差块也将根据输入的嵌入信息进行调节。执行调节的主要方法之一是将嵌入添加到输入中。对于我们的模型，我们将执行线性投影以获得比例和偏差值。然后将输入乘以比例，然后添加偏差。Nichol 和 Dhariwal 表明，与加法相比，使用这种调节方法可以提高 FID 分数。编写此部分代码时需要注意的一点是，调节嵌入的维度可能比输入的维度少。因此，我们需要在嵌入的末尾添加维度。例如，如果输入的形状为 `(B, C, L)` ，而嵌入的形状为 `(B, C)` ，则需要添加一个维度以使其形状为 `(B, C, 1)` 。

```python
# Constructor
self.use_scale_shift = use_scale_shift

self.cond_layers = nn.Sequential(
    nn.SiLU(),
    nn.Linear(model_channels, d_out * 2 if use_scale_shift else d_out)
)

# Forward
emb = self.cond_layers(emb)

while len(emb.shape) < len(x.shape):
    emb = emb[..., None]

if self.use_scale_shift:
    y_s, y_b = emb.chunk(2, dim=1)
    x = y_s * x + y_b
else:
    x += emb
```

调节是在第一次卷积之后进行的，而不是在残差块的开始处进行的，因为它允许模型在使用时间步长信息对其进行细化之前获取基本特征，从而获得更好的表示。

最后，通过将 ResNet 模块的原始输入添加到输出来创建跳过连接。如果需要更改通道数以匹配输出，则对输入执行 1x1 卷积。这些跳跃连接用于增强特征传播。

```python
# Constructor
self.residual = nn.Conv2d(d_in, d_out, 1) if d_in != d_out else nn.Identity()

# Forward
x += self.residual(x_0)
```

将所有这些放在一起，残差块的最终代码应该是这样的：

```python
class ResidualBlock(nn.Module):
    def __init__(self, d_in, d_out, cond_channels=128, n_groups=8, kernel_size=(3,3), dropout=0.0, use_scale_shift=True):
        super().__init__()

        self.use_scale_shift = use_scale_shift

        self.layers1 = nn.Sequential(
            nn.GroupNorm(n_groups, d_in),
            nn.SiLU(),
            nn.Conv2d(d_in, d_out, kernel_size, padding=1)
        )

        # Activation & Linear Projection for Embedding
        self.cond_layers = nn.Sequential(
            nn.SiLU(),
            # d_out multiplied by 2 in order to split into scale & shift if necessary
            nn.Linear(cond_channels, d_out * 2 if use_scale_shift else d_out)
        )

        self.layers2 = nn.Sequential(
            nn.GroupNorm(n_groups, d_out),
            nn.SiLU(),
            nn.Dropout(p=dropout),
            nn.Conv2d(d_out, d_out, kernel_size, padding=1),
            nn.GroupNorm(n_groups, d_out)
        )

        self.residual = nn.Conv2d(d_in, d_out, 1) if d_in != d_out else nn.Identity()

    def forward(self, x_0, emb):
        x = self.layers1(x_0)

        emb = self.cond_layers(emb)

        # Adding dimensions to embedding
        while len(emb.shape) < len(x.shape):
            emb = emb[..., None]

        # Conditioning input with embedding
        if self.use_scale_shift:
            # Getting scale and shift
            y_s, y_b = emb.chunk(2, dim=1)
            # Performing scale and shift
            x = y_s * x + y_b
        else:
            # Adding embedding to input
            x += emb

        x = self.layers2(x)

        # Skip Connection
        x += self.residual(x_0)

        return x
```

## 解码器：注意力块

![[Pasted image 20250906141826.png]]

在我们的模型中，我们将把注意力模块放置在编码器和解码器内层以及瓶颈层（连接编码器和解码器的底层）的残差模块之后。例如，如果编码器和解码器有四层，那么第二层和第三层就会有一个注意力模块。

使用注意力模块对我们的模型有诸多好处。其一是它有助于捕捉图像不同部分之间的空间关系信息。其二是它有助于衡量模型特征的重要性。最后，注意力模块还可以用来调节图像生成过程。

有多种方法可以为注意力模块实现注意力机制。其中一些选项包括自注意力和交叉注意力。我们将使用的方法是 GLIDE 模型中用到的两种方法的混合。对于这种方法，我们将从输入和条件信息中获取键和值，并将它们连接在一起以获得最终的键和值。

```python
Q, K, V = self.qkv(x).chunk(3, dim=-1)
Q = Q.view(B, L, self.n_heads, self.head_size).transpose(1, 2)
K = K.view(B, L, self.n_heads, self.head_size).transpose(1, 2)
V = V.view(B, L, self.n_heads, self.head_size).transpose(1, 2)

# Concatenating keys and values of input and condition
if cond is not None:
    k_c, v_c = self.cond_kv(cond).chunk(2, dim=-1)
    k_c = k_c.view(B, cond.shape[1], self.n_heads, self.head_size).transpose(1, 2)
    v_c = v_c.view(B, cond.shape[1], self.n_heads, self.head_size).transpose(1, 2)
    K = torch.cat((K, k_c), dim=-2)
    V = torch.cat((V, v_c), dim=-2)
```

需要注意的是，输入图像的形状为 `(B, C, H, W)` ，但对于我们的注意力机制，我们希望其形状为 `(B, L, C)` 。因此，我们需要将 `H` 和 `W` 维度合并，并将其与 `C` 维度转置。执行注意力机制后，我们需要将输出转换回其原始形状。

```python
b, c, h, w = x_0.shape

# Changing shape to perform attention
x = x.permute(0, 2, 3, 1).view(b, h * w, c) # (B, C, H, W) -> (B, H * W, C)

# Attention
x = self.attention(x, cond)

# Changing back to original shape
x = x.view(b, h, w, c).permute(0, 3, 1, 2)
```

最终的代码看起来应该是这样的：

```python
class AttentionBlock(nn.Module):
    def __init__(self, n_channels, cond_channels, n_groups=8, n_heads=1, dropout=0.0):
        super().__init__()

        assert n_channels % n_heads == 0, "n_channels must be divisible by n_heads"

        self.n_heads = n_heads
        self.head_size = n_channels // n_heads
        self.scale = self.head_size ** -0.5

        self.group_norm = nn.GroupNorm(n_groups, n_channels)

        self.qkv = nn.Linear(n_channels, n_channels * 3)

        self.cond_kv = nn.Linear(cond_channels, n_channels * 2)

        self.out_proj = nn.Linear(n_channels, n_channels)

        self.dropout = nn.Dropout(dropout)

    def attention(self, x, cond=None):
        B, L, _ = x.shape

        # Getting queries, keys, and values for input
        Q, K, V = self.qkv(x).chunk(3, dim=-1)

        Q = Q.view(B, L, self.n_heads, self.head_size).transpose(1, 2)

        K = K.view(B, L, self.n_heads, self.head_size).transpose(1, 2)

        V = V.view(B, L, self.n_heads, self.head_size).transpose(1, 2)

        # Concatenating keys and values of condition to keys and values of input
        if cond is not None:
            k_c, v_c = self.cond_kv(cond).chunk(2, dim=-1)
            k_c = k_c.view(B, cond.shape[1], self.n_heads, self.head_size).transpose(1, 2)
            v_c = v_c.view(B, cond.shape[1], self.n_heads, self.head_size).transpose(1, 2)
            K = torch.cat((K, k_c), dim=-2)
            V = torch.cat((V, v_c), dim=-2)

        # Get dot product between queries and keys
        attention = torch.matmul(Q, K.transpose(-2, -1))

        # Scale
        attention = attention * self.scale

        # Applying softmax
        attention = torch.softmax(attention, dim=-1)

        # Get dot product with values
        attention = torch.matmul(attention, V)

        # Combine heads
        attention = attention.transpose(1, 2)
        attention = attention.contiguous().view(x.shape)

        # Output projection
        attention = self.out_proj(attention)

        # Dropout
        attention = self.dropout(attention)

        return attention

    def forward(self, x_0, cond=None):
        b, c, h, w = x_0.shape

        # Group normalization
        x = self.group_norm(x_0)

        # Changing shape to perform attention
        x = x.permute(0, 2, 3, 1).view(b, h * w, c) # (B, C, H, W) -> (B, H * W, C)

        # Attention
        x = self.attention(x, cond)

        # Changing back to original shape
        x = x.view(b, h, w, c).permute(0, 3, 1, 2)

        # Residual connection
        x = x + x_0

        return x
```

## Decoder: Downsampling  解码器：下采样

在每个编码器层之间，我们需要降低输入的分辨率。主要的下采样方法有两种：池化和使用卷积层。池化无需参数，这使得它的计算效率更高，并且有助于防止过拟合。卷积层的优势在于它本身带有参数，这使得它能够学习并保留重要的特征。我们将在模型中使用步长 2，这将使输入的分辨率降低 2 倍。

我们将对模型进行编码，以便能够使用任一方法，但是在训练模型时，我们将使用卷积方法。

```python
class Downsample(nn.Module):
    def __init__(self, n_channels, kernel_size=(3,3), stride=2, down_pool=False):
        super().__init__()

        if down_pool:
            self.down = nn.AvgPool2d(stride)
        else:
            self.down = nn.Conv2d(n_channels, n_channels, kernel_size, stride=stride, padding=1)

    def forward(self, x):
        x = self.down(x)
        return x
```

## Decoder: Upsampling  解码器：上采样

在每个解码器层之间，我们需要提高输入的分辨率。为此，我们首先进行插值，将输入的高度和宽度乘以 2。之后，我们将它传入一个卷积层，该层将学习并保留重要特征，同时确保通道数正确。

```python
class Upsample(nn.Module):
    def __init__(self, d_in, d_out, kernel_size=(3,3)):
        super().__init__()

        self.conv = nn.Conv2d(d_in, d_out, kernel_size, padding=1)

    def forward(self, x):
        x = nn.functional.interpolate(x, scale_factor=2)
        x = self.conv(x)
        return x
```

## Decoder: Final Model  解码器：最终模型

在训练解码器时，模型需要做的第一件事就是设置条件信息。为此，我们首先需要从 Prior 模型中采样以获取 CLIP 图像嵌入。与 CLIP 模型一样，在加载 Prior 模型时，应冻结层并将模式设置为 eval。

```python
# Constructor
self.prior = DiffusionPrior(config).to(config.device)
self.prior.load_state_dict(torch.load(config.prior.model_location, map_location=config.device))
freeze_model(self.prior)

# Forward
img_embeddings = self.prior.sample(caption, mask).to(x.device)
```

之后，需要使用输入的时间步长来获取时间步长嵌入（更多信息请参阅上一节的时间步长嵌入）。然后将先前模型生成的 CLIP 图像嵌入投影并添加到时间步长嵌入中。这些嵌入将用于调节模型的残差块。

```python
# Constructor
self.img_projection = nn.Sequential(
    nn.Linear(config.latent_dim, config.decoder.cond_channels),
    nn.SiLU(),
    nn.Linear(config.decoder.cond_channels, config.decoder.cond_channels)
)

#Forward
c_emb = self.time_mlp(time) + self.img_projection(img_embeddings)

for module in self.encoder:
    if isinstance(module, ResidualBlock):
        x = module(x, c_emb)
```

对于注意力条件信息，我们首先需要将文本字幕传入文本 Transformer 编码器。在 unCLIP 论文中，之所以使用这些文本编码，是因为 Ramesh 等人认为这有助于学习 CLIP 无法学习的自然语言知识。但在测试过程中，他们发现这种方法效果不佳，因此这部分是可选的。

```python
# Constructor
self.text_embedding = nn.Embedding(config.vocab_size, config.latent_dim)
self.positional_encodings = nn.Parameter(torch.randn(config.text_seq_length,config.latent_dim) * (config.latent_dim ** -0.5))

self.text_encoder = nn.ModuleList(
    [TransformerBlock(
        config.latent_dim, 
        cond_width=config.latent_dim, 
        n_heads=config.decoder.n_heads, 
        dropout=config.decoder.dropout, 
        r_mlp=config.decoder.r_mlp, 
        bias=config.decoder.bias
     ) for _ in range(config.decoder.text_layers)]
)

self.final_ln = nn.LayerNorm(config.latent_dim)

# Function
def encode_text(self, text, mask=None):
    x = self.text_embedding(text)

    x = x + self.positional_encodings

    for block in self.text_encoder:
        x = block(x, mask=mask)

    x = self.final_ln(x)

    return x

# Forward
text_encodings = self.encode_text(text, mask)
```

对文本进行编码后，CLIP 图像嵌入被投影到四个额外的标记中，并连接到编码文本的末尾。

```python
# Constructor
self.get_img_tokens = nn.Linear(1, config.decoder.n_img_tokens)

# Forward
img_tokens = self.get_img_tokens(img_embeddings[..., None]).permute(0, 2, 1)

c_attn = torch.cat([text_encodings, img_tokens], dim=1)
```

设置条件信息后，噪声图像将通过初始卷积层，以获得初始模型通道的通道数。

```python
# Constructor
ch = config.decoder.model_channels

self.in_conv = nn.Conv2d(config.img_channels, ch, config.decoder.kernel_size, padding=1)

# Forward
x = self.in_conv(x)
```

现在，含噪图像已达到所需的通道数，它们可以与条件信息一起通过 UNet 层。UNet 将包含四个编码器层和解码器层，中间有一个瓶颈层。对于每层的通道数，我们将使用 `[1, 2, 4, 8]` 作为层通道数与模型通道数的比率。UNet 的编码器、解码器和瓶颈层都将包含两个残差块。在瓶颈残差​​块之间以及编码器和解码器内层的每个残差块之后，放置了注意力块（图 4）。跳过连接位于编码器和解码器的残差块之间。

```python
# Config
model_channels:int = 32
channel_ratios:list[int] = field(default_factory=lambda: [1, 2, 4, 8])
n_layer_blocks:int = 2

# Constructor
# UNet Encoder Layers
self.encoder = nn.ModuleList([])
for r in config.decoder.channel_ratios:
    for _ in range(config.decoder.n_layer_blocks):
        self.encoder.append(ResidualBlock(ch, config.decoder.model_channels * r, config.decoder.cond_channels, config.decoder.n_groups, config.decoder.kernel_size, config.decoder.dropout, config.decoder.use_scale_shift))

        ch = config.decoder.model_channels * r
        if r != config.decoder.channel_ratios[0] and r != config.decoder.channel_ratios[-1]:
            self.encoder.append(AttentionBlock(ch, config.latent_dim, config.decoder.n_groups, config.decoder.n_heads, config.decoder.dropout))

    if r != config.decoder.channel_ratios[-1]:
        self.encoder.append(Downsample(ch, config.decoder.kernel_size, config.decoder.stride, config.decoder.down_pool))

# UNet Bottleneck Layers
self.bottleneck = nn.ModuleList([])
for block in range(config.decoder.n_layer_blocks):
    self.bottleneck.append(ResidualBlock(ch, ch, config.decoder.cond_channels, config.decoder.n_groups, config.decoder.kernel_size, config.decoder.dropout, config.decoder.use_scale_shift))

    if block != config.decoder.n_layer_blocks - 1:
        self.bottleneck.append(AttentionBlock(ch, config.latent_dim, config.decoder.n_groups, config.decoder.n_heads, config.decoder.dropout))

# UNet Decoder Layers
self.decoder = nn.ModuleList([])
for r in range(len(config.decoder.channel_ratios))[::-1]:
    for _ in range(config.decoder.n_layer_blocks):
        self.decoder.append(ResidualBlock(ch * 2, ch, config.decoder.cond_channels, config.decoder.n_groups, config.decoder.kernel_size, config.decoder.dropout, config.decoder.use_scale_shift))

        if r != 0 and r!= len(config.decoder.channel_ratios) - 1:
            self.decoder.append(AttentionBlock(ch, config.latent_dim, config.decoder.n_groups, config.decoder.n_heads, config.decoder.dropout))

    if r != 0:
        ch = config.decoder.model_channels * config.decoder.channel_ratios[r-1]
        self.decoder.append(Upsample(config.decoder.model_channels * config.decoder.channel_ratios[r], ch, config.decoder.kernel_size))

# Forward
for module in self.encoder:
    if isinstance(module, ResidualBlock):
        x = module(x, c_emb)
        self.connections.append(x)
    elif isinstance(module, AttentionBlock):
        x = module(x, cond=c_attn)
    else:
        x = module(x)

for module in self.bottleneck:
    if isinstance(module, ResidualBlock):
        x = module(x, c_emb)
    elif isinstance(module, AttentionBlock):
        x = module(x, cond=c_attn)
    else:
        x = module(x)

for module in self.decoder:
    if isinstance(module, ResidualBlock):
        x = torch.cat([x, self.connections.pop()], dim=1)
        x = module(x, c_emb)
    elif isinstance(module, AttentionBlock):
        x = module(x, cond=c_attn)
    else:
        x = module(x)
```

然后，UNet 解码器层的输出会经过 GroupNorm 和 SiLU 激活层，之后使用 Conv2d 让输出恢复到原始通道数。

```python
# Constructor
self.output = nn.Sequential(
    nn.GroupNorm(config.decoder.n_groups, config.decoder.model_channels),
    nn.SiLU(),
    nn.Conv2d(config.decoder.model_channels, config.img_channels, config.decoder.kernel_size, padding=1)
)

# Forward
x = self.output(x)
```

将所有内容放在一起，解码器模型的最终代码将如下所示：

```python
class Decoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Loading Prior Model
        self.prior = DiffusionPrior(config).to(config.device)
        self.prior.load_state_dict(torch.load(config.prior.model_location, map_location=config.device))
        freeze_model(self.prior)

        # MLP to get time embeddings
        self.time_mlp = nn.Sequential(
            SinusoidalPositionalEmbedding(config.decoder.max_time, config.decoder.model_channels),
            nn.Linear(config.decoder.model_channels, config.decoder.cond_channels),
            nn.SiLU(),
            nn.Linear(config.decoder.cond_channels, config.decoder.cond_channels)
        )

        # MLP to project CLIP image embeddings
        self.img_projection = nn.Sequential(
            nn.Linear(config.latent_dim, config.decoder.cond_channels),
            nn.SiLU(),
            nn.Linear(config.decoder.cond_channels, config.decoder.cond_channels)
        )

        # Projection to get image tokens
        self.get_img_tokens = nn.Linear(1, config.decoder.n_img_tokens)

        # Embedding layer for text captions
        self.text_embedding = nn.Embedding(config.vocab_size, config.latent_dim)

        # Learned positional encodings for text captions
        self.positional_encodings = nn.Parameter(torch.randn(config.text_seq_length,config.latent_dim) * (config.latent_dim ** -0.5))

        # Transformer encoder blocks to encoder text captions
        self.text_encoder = nn.ModuleList(
            [TransformerBlock(
                config.latent_dim,
                cond_width=config.latent_dim,
                n_heads=config.decoder.n_heads,
                dropout=config.decoder.dropout,
                r_mlp=config.decoder.r_mlp,
                bias=config.decoder.bias
            ) for _ in range(config.decoder.text_layers)]
        )

        self.final_ln = nn.LayerNorm(config.latent_dim)

        ch = config.decoder.model_channels

        # Initial convolution 
        self.in_conv = nn.Conv2d(config.img_channels, ch, config.decoder.kernel_size, padding=1)

        # UNet Encoder Layers
        self.encoder = nn.ModuleList([])
        for r in config.decoder.channel_ratios:
            for _ in range(config.decoder.n_layer_blocks):
                self.encoder.append(ResidualBlock(ch, config.decoder.model_channels * r, config.decoder.cond_channels, config.decoder.n_groups, config.decoder.kernel_size, config.decoder.dropout, config.decoder.use_scale_shift))

                ch = config.decoder.model_channels * r
                if r != config.decoder.channel_ratios[0] and r != config.decoder.channel_ratios[-1]:
                    self.encoder.append(AttentionBlock(ch, config.latent_dim, config.decoder.n_groups, config.decoder.n_heads, config.decoder.dropout))

            if r != config.decoder.channel_ratios[-1]:
                self.encoder.append(Downsample(ch, config.decoder.kernel_size, config.decoder.stride, config.decoder.down_pool))

        # UNet Bottleneck Layers
        self.bottleneck = nn.ModuleList([])
        for block in range(config.decoder.n_layer_blocks):
            self.bottleneck.append(ResidualBlock(ch, ch, config.decoder.cond_channels, config.decoder.n_groups, config.decoder.kernel_size, config.decoder.dropout, config.decoder.use_scale_shift))

            if block != config.decoder.n_layer_blocks - 1:
                self.bottleneck.append(AttentionBlock(ch, config.latent_dim, config.decoder.n_groups, config.decoder.n_heads, config.decoder.dropout))

        # UNet Decoder Layers
        self.decoder = nn.ModuleList([])
        for r in range(len(config.decoder.channel_ratios))[::-1]:
            for _ in range(config.decoder.n_layer_blocks):
                self.decoder.append(ResidualBlock(ch * 2, ch, config.decoder.cond_channels, config.decoder.n_groups, config.decoder.kernel_size, config.decoder.dropout, config.decoder.use_scale_shift))

                if r != 0 and r!= len(config.decoder.channel_ratios) - 1:
                    self.decoder.append(AttentionBlock(ch, config.latent_dim, config.decoder.n_groups, config.decoder.n_heads, config.decoder.dropout))

            if r != 0:
                ch = config.decoder.model_channels * config.decoder.channel_ratios[r-1]
                self.decoder.append(Upsample(config.decoder.model_channels * config.decoder.channel_ratios[r], ch, config.decoder.kernel_size))

        # Output projection
        self.output = nn.Sequential(
            nn.GroupNorm(config.decoder.n_groups, config.decoder.model_channels),
            nn.SiLU(),
            nn.Conv2d(config.decoder.model_channels, config.img_channels, config.decoder.kernel_size, padding=1)
        )

        # Skip connections
        self.connections = []

    def encode_text(self, text, mask=None):
        x = self.text_embedding(text)

        x = x + self.positional_encodings

        for block in self.text_encoder:
            x = block(x, mask=mask)

        x = self.final_ln(x)

        return x

    def forward(self, x, time, caption=None, mask=None):
        # Sample prior model to get CLIP image embeddings
        img_embeddings = self.prior.sample(caption, mask).to(x.device)

        # Get conditioning information for residual blocks
        c_emb = self.time_mlp(time) + self.img_projection(img_embeddings)

        # Get conditioning information for attention blocks
        c_attn = self.get_img_tokens(img_embeddings[..., None]).permute(0, 2, 1)
        if caption is not None:
            c_attn = torch.cat([self.encode_text(caption, mask), c_attn], dim=1)

        # Initial convolution
        x = self.in_conv(x)

        # UNet encoder layers
        for module in self.encoder:
            if isinstance(module, ResidualBlock):
                x = module(x, c_emb)
                self.connections.append(x)
            elif isinstance(module, AttentionBlock):
                x = module(x, cond=c_attn)
            else:
                x = module(x)

        # UNet bottleneck layers
        for module in self.bottleneck:
            if isinstance(module, ResidualBlock):
                x = module(x, c_emb)
            elif isinstance(module, AttentionBlock):
                x = module(x, cond=c_attn)
            else:
                x = module(x)

        # UNet decoder layers
        for module in self.decoder:
            if isinstance(module, ResidualBlock):
                x = torch.cat([x, self.connections.pop()], dim=1)
                x = module(x, c_emb)
            elif isinstance(module, AttentionBlock):
                x = module(x, cond=c_attn)
            else:
                x = module(x)

        # Output projection
        x = self.output(x)

        return x
```

## Decoder: Loss  解码器：损失

训练解码器模型的一个重要部分是损失函数。我们的损失函数首先要做的就是对批次中的每个图像进行随机时间步长的采样。

```python
timesteps = torch.randint(0, config.decoder.max_time, (image.shape[0],), device=config.device, dtype=torch.long)
```

然后使用这些时间步长和计划值来获取通过前向扩散添加的噪声图像和噪声。

```python
schedule_values = get_schedule_values(config)
noisy_image, noise = forward_diffusion(image, schedule_values, timesteps)
```

然后将噪声图像、时间步长、文本标题和文本掩码传递到模型中以获得预测噪声。

```python
pred_noise = decoder(noisy_image, timesteps, caption, mask)
```

最后，我们可以使用模型的预测噪声和前向扩散函数的实际噪声来计算损失。我们将用于解码器的损失是预测噪声和实际噪声之间的均方误差损失。

```python
loss = nn.functional.mse_loss(pred_noise, noise)
```

综合起来，我们得到：

```python
# Calculating Loss
get_schedule_values(config)
timesteps = torch.randint(0, config.decoder.max_time, (image.shape[0],), device=config.device, dtype=torch.long)
noisy_image, noise = forward_diffusion(image, schedule_values, timesteps)
pred_noise = decoder(noisy_image, timesteps, caption, mask)
loss = nn.functional.mse_loss(pred_noise, noise)
```

## 配置

```python
@dataclass
class CLIPConfig:
    # Vision Transformer
    patch_size:tuple[int,int] = (4,4)
    vit_width:int = 256
    vit_layers:int = 6
    vit_heads:int = 8
    # Text Transformer
    text_width:int = 256
    text_layers:int = 6
    text_heads:int = 8
    # Attention
    dropout:float = 0.2
    r_mlp:int = 4
    bias:bool = False
    # Training
    augment_data:bool = True
    num_workers:int = 0
    batch_size:int = 128
    lr:float = 5e-4
    lr_min:float = 1e-5
    weight_decay:float = 1e-4
    epochs:int = 200
    warmup_epochs:int = 5
    grad_max_norm:float = 1.0
    get_val_accuracy:bool = False
    model_location:str = "../clip_fmnist.pt"

@dataclass
class PriorConfig:
    # Diffusion
    max_time:int = 1000
    schedule:str = "cosine"
    schedule_offset:float = 0.008
    # Transformer Decoder
    width:int = 256
    n_layers:int = 6
    n_heads:int = 8
    # Attention
    dropout:float = 0.2
    r_mlp:int = 4
    bias:bool = False
    # Training
    augment_data:bool = False
    num_workers:int = 0
    batch_size:int = 128
    lr:float = 5e-4
    lr_min:float = 1e-5
    weight_decay:float = 1e-4
    epochs:int = 150
    warmup_epochs:int = 5
    grad_max_norm:float = 1.0
    model_location:str = "../prior_fmnist.pt"

@dataclass
class DecoderConfig:
    # Diffusion
    max_time:int = 1000
    schedule:str = "cosine"
    # UNet
    n_groups:int = 8
    kernel_size:tuple[int, int] = (3,3)
    model_channels:int = 32
    cond_channels:int = 128
    channel_ratios:list[int] = field(default_factory=lambda: [1, 2, 4, 8])
    n_layer_blocks:int = 2
    dropout:float = 0.1
    use_scale_shift:bool = True
    n_heads:int = 8
    stride:int = 2
    down_pool:bool = False
    r_mlp:int = 4
    bias:bool = False
    text_layers:int = 4
    n_img_tokens:int = 4
    # Training
    augment_data:bool = False
    num_workers:int = 0
    batch_size:int = 32
    lr:float = 5e-4
    lr_min:float = 1e-5
    weight_decay:float = 1e-4
    epochs:int = 100
    warmup_epochs:int = 5
    grad_max_norm:float = 1.0
    sample_after_epoch:bool = True
    model_location:str = "../decoder_fmnist.pt"

@dataclass
class FMNISTConfig:
    latent_dim:int = 256
    # Dataset Info
    dataset:str = "fashion_mnist"
    data_location:str = "./../datasets"
    img_size:tuple[int,int] = (32,32)
    img_channels:int = 1
    vocab_size:int = 256
    text_seq_length:int = 64
    # Data Augmentation / Normalization
    prob_hflip:float = 0.5
    crop_padding:int = 4
    train_mean:list[float] = field(default_factory=lambda: [0.2855552])
    train_std:list[float] = field(default_factory=lambda: [0.33848408])
    # Training
    train_val_split:tuple[int,int] = (50000, 10000)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # Model Configs
    clip = CLIPConfig()
    prior = PriorConfig()
    decoder = DecoderConfig()
```

## 训练

对于这个文本到图像的扩散模型，我们实际上需要训练三个独立的模型：CLIP、扩散先验和解码器。

对于数据集，所有图像的尺寸都从 (28,28) 调整为 (32,32)。图像也进行了归一化，平均值和标准差设置为训练样本的平均值和标准差。

```python
transform = T.Compose([
    T.Resize(config.img_size),
    T.ToTensor(),
    T.Normalize(config.train_mean, config.train_std)
])
```

对于 CLIP 模型的训练集，我还通过随机水平翻转和随机裁剪实现了训练分割的数据增强。

```python
transform = T.Compose([
    T.Resize(config.img_size),
    T.RandomHorizontalFlip(p=config.prob_hflip)
    T.RandomCrop(config.img_size[0], padding=config.crop_padding)
    T.ToTensor(),
    T.Normalize(config.train_mean, config.train_std)
])
```

在训练这三个模型时，当使用权重衰减时，我使用 AdamW 作为优化器，否则使用 Adam。

```python
if config.clip.weight_decay == 0:
    optimizer = Adam(clip.parameters(), lr=config.clip.lr)
else:
    optimizer = AdamW(clip.parameters(), lr=config.clip.lr, weight_decay=config.clip.weight_decay)
```

对于所有模型，我还使用了带有线性预热的余弦退火学习率调度器。该调度器在每个 epoch 结束时更新。

```python
if config.clip.warmup_epochs > 0:
    warmup = lr_scheduler.LinearLR(optimizer=optimizer, start_factor=(1 / config.clip.warmup_epochs), end_factor=1.0, total_iters=(config.clip.warmup_epochs - 1), last_epoch=-1)

scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=(config.clip.epochs - config.clip.warmup_epochs), eta_min=config.clip.lr_min)
```

所有模型还使用了最大范数为 1.0 的梯度剪裁，以帮助防止梯度爆炸。

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.[model].grad_max_norm)
```

在训练 CLIP 模型时，训练数据被随机分成训练集和验证集。当某个 epoch 的验证损失小于或等于前一个最低验证损失时，模型权重会被保存。

## 采样

![[Pasted image 20250906142959.png]]

采样过程的第一步是随机生成仅包含噪声的图像。

```python
B = prompts.shape[0]
# Get completely noisy image
img = torch.randn((B, config.img_channels, config.img_size[0], config.img_size[1]), device=config.device)
```

得到噪声图像后，我们将计算一些需要计算 $x_{t-1}$ 的时间表值。

```python
def get_schedule_values(config):
        schedule_values = {}
        schedule_values["betas"] = get_beta_schedule(config.decoder.schedule, config.decoder.max_time).to(config.device)
        schedule_values["alphas"] = 1.0 - schedule_values["betas"]
        schedule_values["alpha_bars"] = torch.cumprod(schedule_values["alphas"], axis = 0)
        schedule_values["sqrt_recip_alphas"] = torch.sqrt(1.0 / schedule_values["alphas"])
        schedule_values["sqrt_alpha_bars"] = torch.sqrt(schedule_values["alpha_bars"])
        schedule_values["sqrt_one_minus_alpha_bars"] = torch.sqrt(1.0 - schedule_values["alpha_bars"])
        schedule_values["alpha_bars_prev"] = torch.cat((torch.ones(1, device=config.device), schedule_values["alpha_bars"][:-1]))
        schedule_values["sigma"] = schedule_values["betas"] * (1.0 - schedule_values["alpha_bars_prev"]) / (1.0 - schedule_values["alpha_bars"])
        return schedule_values

schedule_values = get_schedule_values(config)
```

然后，我们需要以相反的顺序迭代遍历所有时间步长，从 0 到 max_time-1。我们还需要扩展时间步长，以便批次中的每个项目都有一个时间步长。

```python
for t in range(0, config.max_time)[::-1]:
    timesteps = torch.full((B,), t, device=config.device, dtype=torch.long)
```

对于每个时间步的采样，第一步是获取该时间步的调度值。获取该时间步的调度值后，需要对其进行扩展，使其维度数等于图像的维度数。

```python
# Getting schedule values for timestep
sqrt_recip_alphas_t = extract_and_expand(schedule_values["sqrt_recip_alphas"], timesteps, img.shape)
betas_t = extract_and_expand(schedule_values["betas"], timesteps, img.shape)
sqrt_one_minus_alpha_bars_t = extract_and_expand(schedule_values["sqrt_one_minus_alpha_bars"], timesteps, img.shape)
sigma_t = extract_and_expand(schedule_values["sigma"], timesteps, img.shape)
```

之后，我们使用解码器模型来预测时间步长 t 的噪声。

```python
# Predicting noise at timestep t with decoder
pred_noise = decoder(img, timesteps, caption=prompt, mask=mask)
```

如果不是最终时间步，我们还需要生成随机噪声 (z)。我们需要这个值和 `sigma_t` ，因为我们的模型直接预测从 $x_t$ 到 $x_0$ 的噪声，而不是从 $x_t$ 到 $x_{t-1}$ 的噪声。因此，模型会预测从 $x_t$ 到 $x_0$ 的噪声，并将从 $x_{t-1}$ 到 $x_0$ 的噪声加回去。这样一来，只有从 $x_t$ 到 $x_{t-1}$ 的噪声被移除。最后一步不需要 `sigma_t` 和 z 值，因为从 $x_t$ 到 $x_0$ 的预测噪声与从 $x_t$ 到 $x_{t-1}$ 的预测噪声相同。

```python
# Generating random noise
z = torch.randn_like(img) if t > 0 else 0
```

利用生成的噪声、计划值和预测噪声，我们可以计算时间步长 t-1 的图像。

```python
# Calculating image at timestep t-1
img = sqrt_recip_alphas_t * (img - (betas_t / sqrt_one_minus_alpha_bars_t) * pred_noise) + (sigma_t * z)
img = torch.clamp(img, -1.0, 1.0)
```

综合起来，采样和图像的代码将如下所示：

```python
@torch.no_grad()
def sample_image(config, prompt, mask, schedule_values=None):
    # Load decoder model
    decoder = Decoder(config).to(config.device)
    decoder.load_state_dict(torch.load(config.decoder.model_location, map_location=config.device))
    decoder.eval()

    B = prompt.shape[0]
    # Get completely noisy image
    img = torch.randn((B, config.img_channels, config.img_size[0], config.img_size[1]), device=config.device)

    # Calculate schedule values
    if schedule_values is None:
        schedule_values = get_schedule_values(config)

    for t in range(0, config.decoder.max_time)[::-1]:
        # Setting the timesteps for all the items in the batch
        timesteps = torch.full((B,), t, device=config.device, dtype=torch.long)

        # Getting schedule values for timestep
        sqrt_recip_alphas_t = extract_and_expand(schedule_values["sqrt_recip_alphas"], timesteps, img.shape)
        betas_t = extract_and_expand(schedule_values["betas"], timesteps, img.shape)
        sqrt_one_minus_alpha_bars_t = extract_and_expand(schedule_values["sqrt_one_minus_alpha_bars"], timesteps, img.shape)
        sigma_t = extract_and_expand(schedule_values["sigma"], timesteps, img.shape)

        # Predicting noise at timestep t with decoder
        pred_noise = decoder(img, timesteps, caption=prompt, mask=mask)

        # Generating random noise
        z = torch.randn_like(img) if t > 0 else 0

        # Calculating image at timestep t-1
        img = sqrt_recip_alphas_t * (img - (betas_t / sqrt_one_minus_alpha_bars_t) * pred_noise) + (sigma_t * z)

        img = torch.clamp(img, -1.0, 1.0)

    return img
```

## 结果

为了查看模型的结果，我将展示每个字幕的反向扩散过程。为了查看这一点，我们创建了一个修改版的 `sample_image` 函数，用于绘制反向扩散过程中10个时间步的图像。

```python
# Displaying Results
config = FMNISTConfig()
captions = {
    0: "An image of a t-shirt/top",
    1: "An image of trousers",
    2: "An image of a pullover",
    3: "An image of a dress",
    4: "An image of a coat",
    5: "An image of a sandal",
    6: "An image of a shirt",
    7: "An image of a sneaker",
    8: "An image of a bag",
    9: "An image of an ankle boot"
}
sample_captions = torch.stack([tokenizer(x, text_seq_length=config.text_seq_length)[0] for x in captions.values()]).to(config.device)
sample_masks = torch.stack([tokenizer(x, text_seq_length=config.text_seq_length)[1] for x in captions.values()]).to(config.device)
for i in range(len(sample_captions)):
    caption = sample_captions[None, (i % len(sample_captions))]
    mask = sample_masks[None, (i % len(sample_masks))]
    test = sample_plot_image(config, caption, mask)
```

![[Pasted image 20250906143451.png]]

