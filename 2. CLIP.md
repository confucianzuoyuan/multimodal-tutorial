
计算机视觉系统历来仅限于一组固定的类别，CLIP 是一场革命，它允许通过“预测哪些图像和文本配对在一起”来识别开放世界中的对象。CLIP 能够通过学习批量训练数据的图像和文本特征之间的余弦相似度来预测这一点。这在下图的对比预训练部分显示，其中图像之间的点积特征 $\{I_1, I_2, \cdots, I_N\}$ 和文本特征 $\{T_1,T_2,\cdots,T_N\}$ 被占用。

![[Pasted image 20250826173300.png]]

在本教程中，我们将从头开始构建 CLIP 并在 Fashion-MNIST 数据集上对其进行测试。

## 导入库和模块

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
import matplotlib.pyplot as plt
import numpy as np
```

我们将使用 PyTorch 构建 CLIP，因此我们需要导入库以及我们将在本教程中使用的其他库。

```python
import torch  
import torch.nn as nn
```

我们还需要导入 `torchvision.transforms` 以调整输入图像的大小并将它们转换为张量。调整输入图像的大小是可选的。只需要确保图像大小可以被 `patch` （补丁）的大小整除。

```python
import torchvision.transforms as T
```

我们将使用 `Adam` 作为优化器，因此我们需要从 `torch.optim` 导入它。

```python
from torch.optim import Adam
```

在本教程中，我们将从 HuggingFace 导入 Fashion-MNIST 数据集，因此我们需要导入数据集。我们将使用 PyTorch 中的 `DataLoader` 来帮助加载数据，因此我们也需要将其导入。

```python
from datasets import load_dataset
from torch.utils.data import DataLoader
```

我们将导入 `matplotlib` ，以便在进行 Zero-Shot 分类时显示图像。

```python
import matplotlib.pyplot as plt
```

最后，我们需要导入 `numpy` ，在创建位置编码时，我们将使用它来执行正弦和余弦运算。

## 图像和文本编码器

我们将首先构建图像和文本编码器。两者分别将图像和文本嵌入到单个 token 中，然后可用于对比损失计算。

### 位置嵌入

```python
class PositionalEmbedding(nn.Module):
  def __init__(self, width, max_seq_length):
    super().__init__()

    pe = torch.zeros(max_seq_length, width)

    for pos in range(max_seq_length):
      for i in range(width):
        if i % 2 == 0:
          pe[pos][i] = np.sin(pos/(10000 ** (i/width)))
        else:
          pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/width)))

    self.register_buffer('pe', pe.unsqueeze(0))

  def forward(self, x):
    x = x + self.pe
    return x
```

![[Pasted image 20250826174333.png]]

与 LSTM 等按顺序接受嵌入的模型不同，Transformer 并行的接受嵌入。虽然这提高了速度，但 Transformer 不知道序列的顺序是什么。这是一个很大的问题，因为改变序列的顺序很可能会改变其含义。上图就是一个例子，它显示更改图像的补丁顺序可以将图像从 O 更改为更接近 X 的图像。为了解决这个问题，需要将位置编码添加到嵌入中。每个位置编码对于它所表示的位置都是唯一的，这允许模型识别每个嵌入应该去的位置。为了将位置编码添加到嵌入中，它们必须具有相同的维度，`d_model` 。我们使用下面的公式获得位置编码。

$$
\begin{aligned}
\text{PE}_{(\text{pos},2i)} &= \sin(\frac{\text{pos}}{10000^{\frac{2i}{d_{model}}}}) \\
\text{PE}_{(\text{pos},2i+1)} &= \cos(\frac{\text{pos}}{10000^{\frac{2i}{d_{model}}}})
\end{aligned}
$$


```python
pe = torch.zeros(max_seq_length, width)

for pos in range(max_seq_length):
  for i in range(width):
    if i % 2 == 0:
      pe[pos][i] = np.sin(pos/(10000 ** (i/width)))
    else:
      pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/width)))

self.register_buffer('pe', pe.unsqueeze(0))
```

请注意，`pe` 只是一个局部变量，并且只能使用 `register_buffer` 方法添加到类中。这样，位置编码就成为模型中不可训练的部分。

在 `forward` 方法中，我们上面计算的位置编码被添加到输入中。

```python
x = x + self.pe

return x
```

### 注意力头

```python
class AttentionHead(nn.Module):
  def __init__(self, width, head_size):
    super().__init__()
    self.head_size = head_size

    self.query = nn.Linear(width, head_size)
    self.key = nn.Linear(width, head_size)
    self.value = nn.Linear(width, head_size)

  def forward(self, x, mask=None):
    # Obtaining Queries, Keys, and Values
    Q = self.query(x)
    K = self.key(x)
    V = self.value(x)

    # Dot Product of Queries and Keys
    attention = Q @ K.transpose(-2,-1)

    # Scaling
    attention = attention / (self.head_size ** 0.5)

    # Applying Attention Mask
    if mask is not None:
        attention = attention.masked_fill(mask == 0, float("-inf"))

    attention = torch.softmax(attention, dim=-1)

    attention = attention @ V

    return attention
```

![[Pasted image 20250826175202.png]]

`Transformer` 使用注意力机制，这是一种通信机制，允许模型专注于图像的重要部分。可以使用下面的公式计算注意力分数。

$$
\text{Attention}(Q,K,V)=\text{softmax}\left({\frac{QK^T}{\sqrt{d_k}}}\right)V
$$

计算注意力的第一步是获取 token 的查询、键和值。token 的查询是 token 要查找的内容，键是 token 包含的内容，值是 token 之间通信的内容。查询、键和值可以通过线性层传递 token 来计算。

```python
def forward(self, x):
   # Obtaining Queries, Keys, and Values
   Q = self.query(x)
   K = self.key(x)
   V = self.value(x)
```

我们能够通过获取查询和键的点积来获取序列中 token 之间的关系。

```python
# Dot Product of Queries and Keys
attention = Q @ K.transpose(-2,-1)
```

我们需要缩放这些值以控制初始化时的方差，以便 token 能够聚合来自多个其他 token 的信息。通过将点积除以注意力头大小的平方根来应用缩放。

```python
# Scaling
attention = attention / (self.head_size ** 0.5)
```

Transformer 编码器和解码器之间的主要区别在于解码器应用注意力掩码，而编码器则不应用。虽然 CLIP 是仅编码器模型，但由于在tokennize期间应用于输入文本的填充，因此仍需要与文本编码器一起应用掩码。请注意，掩码是可选的，因此此注意力头可用于文本和视觉编码器。

![[Pasted image 20250826175813.png]]

```python
# Applying Attention Mask
if mask is not None:
    attention = attention.masked_fill(mask == 0, float("-inf"))
```

然后，我们需要对缩放的点积应用 `softmax` 运算。在这里，具有负无穷大的值将被简单地忽略。

```python
attention = torch.softmax(attention, dim=-1)
```

最后，我们需要获取 `softmax` 和值矩阵之间的点积。这本质上是在相应的 token 之间传递信息。

```python
attention = attention @ V

return attention
```

### 多头注意力

```python
class MultiHeadAttention(nn.Module):
  def __init__(self, width, n_heads):
    super().__init__()
    self.head_size = width // n_heads

    self.W_o = nn.Linear(width, width)

    self.heads = nn.ModuleList([AttentionHead(width, self.head_size) for _ in range(n_heads)])

  def forward(self, x, mask=None):
    # Combine attention heads
    out = torch.cat([head(x, mask=mask) for head in self.heads], dim=-1)

    out = self.W_o(out)

    return out
```

多头注意力只是并行运行多个头的自注意力并将它们组合起来。我们可以通过将注意力头添加到模块列表中来做到这一点，

```python
self.heads = nn.ModuleList([AttentionHead(width, self.head_size) for _ in range(n_heads)])
```

传递输入并连接结果。

```python
def forward(self, x, mask=None):
    # Combine attention heads
    out = torch.cat([head(x, mask=mask) for head in self.heads], dim=-1)
```

然后，我们需要将输出传递给另一个线性层。

```python
out = self.W_o(out)

return out
```

### Transformer 编码器

```python
class TransformerEncoder(nn.Module):
    def __init__(self, width, n_heads, r_mlp=4):
        super().__init__()
        self.width = width
        self.n_heads = n_heads

        # Sub-Layer 1 Normalization
        self.ln1 = nn.LayerNorm(width)

        # Multi-Head Attention
        self.mha = MultiHeadAttention(width, n_heads)

        # Sub-Layer 2 Normalization
        self.ln2 = nn.LayerNorm(width)

        # Multilayer Perception
        self.mlp = nn.Sequential(
            nn.Linear(self.width, self.width*r_mlp),
            nn.GELU(),
            nn.Linear(self.width*r_mlp, self.width)
        )


    def forward(self, x, mask=None):
        x = x + self.mha(self.ln1(x), mask=mask)

        x = x + self.mlp(self.ln2(x))

        return x
```

![[Pasted image 20250826180251.png]]

Transformer 编码器由两个子层组成：第一个子层执行多头注意力，第二个子层包含多层感知器。多头注意力子层执行 token 之间的通信，而多层感知器子层允许 token 单独“思考”与它们通信的内容。

层归一化是一种优化技术，可跨其特征独立归一化批处理中的每个输入。对于我们的模型，我们将在每个子层的开头通过层归一化模块传递我们的输入。

```python
# Sub-Layer 1 Normalization
self.ln1 = nn.LayerNorm(width)

# Sub-Layer 2 Normalization
self.ln2 = nn.LayerNorm(width)
```

MLP 将由两个线性层组成，中间有一个 GELU 层。使用 GELU 代替 RELU，因为它没有 RELU 在零处不可导的限制。

```python
# Multilayer Perception
self.mlp = nn.Sequential(
    nn.Linear(width, width*r_mlp),
    nn.GELU(),
    nn.Linear(width*r_mlp, width)
)
```

在编码器的 `forward` 方法中，输入在执行多头注意力之前通过第一层归一化模块。通过执行多头注意力将原始输入添加到输出中，以创建残差连接。

然后，在输入 MLP 之前，它会通过另一个层归一化模块。通过将 MLP 的输出添加到第一个残差连接的输出，创建另一个残差连接。

残差连接用于通过创建一条路径来帮助防止梯度消失问题，以便梯度不受阻碍地反向传播回原始输入。

```python
def forward(self, x):
    # Residual Connection After Sub-Layer 1
    out = x + self.mha(self.ln1(x))

    # Residual Connection After Sub-Layer 2
    out = out + self.mlp(self.ln2(out))

    return out
```

### 分词

```python
def tokenizer(text, encode=True, mask=None, max_seq_length=32):
    if encode:
        out = chr(2) + text + chr(3) # Adding SOT and EOT tokens
        out = out + "".join([chr(0) for _ in range(max_seq_length-len(out))]) # Adding Padding
        out = torch.IntTensor(list(out.encode("utf-8"))) # Encoding Text
        mask = torch.ones(len(out.nonzero()))
        mask = torch.cat((mask,torch.zeros(max_seq_length-len(mask)))).type(torch.IntTensor)
    else:
        out = [chr(x) for x in text[1:len(mask.nonzero())-1]]
        out = "".join(out)
        mask = None

    return out, mask
```

Transformer 无法处理原始文本，因此我们需要做的第一件事是在将输入字符串通过文本编码器之前对其进行分词。

在本教程中，我们将进行一个简单版本的分词，其中我们只使用 UTF-8 编码。为什么使用 UTF-8 编码进行分词？因为我们只会在示例中使用简单的ascii码文本。对于更复杂的示例，可能需要使用 BPE 分词器。这是因为使用 UTF-8 编码时，最大词汇量（`vocab_size`）为 256，这意味着对于更复杂的示例，可能会有更长的输入序列，由于上下文长度有限，这在进行注意力时效率低下。

![[Pasted image 20250826180807.png]]

分词器的第一步是将文本的开头和文本的结尾token添加到输入字符串中。

```python
text = chr(2) + text + chr(3)
```

添加文本的开头和文本的结尾token后，我们需要将序列的长度填充到最大序列长度。

```python
text = text + "".join([chr(0) for _ in range(10-len(text))])
```

我们通过将文本序列编码为 UTF-8 并将输出转换为 `IntTensor` 来完成分词。

```python
text = torch.IntTensor(list(text.encode("utf-8")))
```

对文本进行分词后，我们需要为文本创建一个掩码。虽然 Transformer 中通常使用的掩码用于确保 token 不与未来的 token 通信，但我们在这里应用的掩码只是使填充 token 被忽略。因此，掩码将只是一个大小等于最大序列长度的张量，其中元素在有填充的情况下为 0，否则为 1。

```python
mask = torch.ones(len(text.nonzero()))
mask = torch.cat((mask,torch.zeros(10-len(mask)))).type(torch.IntTensor)
```

### 文本编码器

```python
class TextEncoder(nn.Module):
    def __init__(self, vocab_size, width, max_seq_length, n_heads, n_layers, emb_dim):
        super().__init__()
        self.max_seq_length = max_seq_length
        self.encoder_embedding = nn.Embedding(vocab_size, width)
        self.positional_embedding = PositionalEmbedding(width, max_seq_length)
        self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])

        # learned proj of image to embed
        self.projection = nn.Parameter(torch.randn(width, emb_dim))

    def forward(self, text, mask=None):
        # Text Embedding
        x = self.encoder_embedding(text)

        # Positional Embedding
        x = self.positional_embedding(x)

        # Transformer Encoder
        for encoder_layer in self.encoder:
            x = encoder_layer(x, mask=mask)

        # Takes features from the EOT Embedding
        x = x[torch.arange(text.shape[0]), torch.sub(torch.sum(mask[:,0],dim=1),1)]

        # joint multimodal embedding
        if self.projection is not None:
            x = x @ self.projection

        x = x / torch.norm(x, dim=-1, keepdim=True)

        return x
```

对于文本编码器，我们将使用常规 Transformer 模型。创建文本编码器的第一步是创建大小 `（vocab_size, width)` 的嵌入表。此嵌入表包含一个向量表示，其大小等于词汇表中每个 token 的 Transformer 模型的 `width` 。

```python
self.encoder_embedding = nn.Embedding(vocab_size, width)
```

在输出 Transformer 的结果之前，我们需要将特征嵌入到联合嵌入空间中。我们将通过获取文本特征的点积以及使用 `nn.Parameter` 创建的学习到的投影来实现这一点。

```python
# learned proj of image to embed
self.projection = nn.Parameter(torch.randn(width, emb_dim))
```

在 `forward` 方法中，我们要做的第一件事是通过嵌入表传递文本 token 。

```python
# Text Embedding
x = self.encoder_embedding(text)
```

然后，我们需要将位置编码添加到嵌入表的输出中。

```python
# Positional Embedding
x = self.positional_embedding(x)
```

添加位置编码后，我们现在可以将其与掩码一起通过编码器层。

```python
# Transformer Encoder
for encoder_layer in self.encoder:
    x = encoder_layer(x, mask=mask)
```

编码器层的输出是文本的特征。我们将使用 `EOT` 嵌入的功能。

```python
# Takes features from the EOT Embedding
x = x[torch.arange(text.shape[0]),torch.sub(torch.sum(mask[:,0],dim=1),1)]
```

最后，我们通过获取特征和学习到的投影之间的点积，将文本特征嵌入到联合嵌入空间中，并通过除以归一化的点积对其进行归一化。

```python
# joint multimodal embedding
if self.projection is not None:
    x = x @ self.projection

x = x / torch.norm(x, dim=-1, keepdim=True)

return x
```

### 图像编码器

```python
class ImageEncoder(nn.Module):
    def __init__(self, width, img_size, patch_size, n_channels, n_layers, n_heads, emb_dim):
        super().__init__()

        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, "img_size dimensions must be divisible by patch_size dimensions"
        assert width % n_heads == 0, "width must be divisible by n_heads"

        self.n_patches = (img_size[0] * img_size[1]) // (patch_size[0] * patch_size[1])

        self.max_seq_length = self.n_patches + 1

        self.linear_project = nn.Conv2d(n_channels, width, kernel_size=patch_size, stride=patch_size)

        self.cls_token = nn.Parameter(torch.randn(1, 1, width))

        self.positional_embedding = PositionalEmbedding(width,self.max_seq_length)

        self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])


        # learned proj of image to embed
        self.projection = nn.Parameter(torch.randn(width, emb_dim))


    def forward(self,x):
        # Patch Embedding
        x = self.linear_project(x)
        x = x.flatten(2).transpose(1, 2)

        # Positional Embedding
        x = torch.cat((self.cls_token.expand(x.size()[0], -1, -1),x), dim=1)
        x = self.positional_embedding(x)

        # Transformer Encoder
        for encoder_layer in self.encoder:
            x = encoder_layer(x)

        # Getting Class Tokens
        x = x[:, 0, :]

        # joint multimodal embedding
        if self.projection is not None:
            x = x @ self.projection

        x = x / torch.norm(x, dim=-1, keepdim=True)

        return x
```

![[Pasted image 20250826182650.png]]

对于图像编码器，我们将使用 `VisionTransformer` 。在创建图像编码器时，我们首先需要确保输入图像可以均匀地分割成大小为 `patch_size` 的补丁，并且模型的维数可以被注意力头的数量整除。

```python
assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, "img_size dimensions must be divisible by patch_size dimensions"
assert width % n_heads == 0, "width must be divisible by n_heads"
```

我们还需要计算位置编码的最大序列长度，该长度将等于补丁数加一。可以通过将输入图像的高度和宽度的乘积除以补丁大小的高度和宽度的乘积来求出补丁的数量。

```python
self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])
self.max_seq_length = self.n_patches + 1
```

`VisionTransformer` 还需要能够具有多个编码器模块。这可以通过在 ModuleList 中放置编码器层列表来实现。

```python
self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])
```

在通过编码器层传递输入之前，我们首先需要将输入图像拆分为补丁，并创建这些补丁的线性嵌入序列。我们能够通过使用 PyTorch 的 `Conv2d` 方法来实现这一点

`Conv2d` 方法获取输入图像，将它们拆分为补丁，并提供大小等于模型宽度的线性投影。通过将 `kernel_size` 和步幅设置为补丁大小，我们确保补丁大小正确且没有重叠。

```python
self.linear_project = nn.Conv2d(n_channels, width, kernel_size=patch_size, stride=patch_size)
```

在 `forward` 方法中，我们通过 `linear_project/Conv2D` 方法传递具有形状 `(B,C,H,W)` 的输入，并接收形状 `(B,d_model, P_col, P_row)` 的输出。

```python
def forward(self, x):
    x = self.linear_project(x) # (B, C, H, W) -> (B, width, P_col, P_row)
```

![[Pasted image 20250826183421.png]]

我们使用展平方法将补丁列和补丁行维度组合成一个补丁维度，从而得到 `(B, d_model, P)` 的形状

```python
x = x.flatten(2) # (B, width, P_col, P_row) -> (B, width, P)
```

![[Pasted image 20250826183538.png]]

最后，我们使用转置方法切换 `d_model` 和补丁维度，得到 `(B, P, d_model)` 的形状。

```python
x = x.transpose(-2, -1) # (B, width, P) -> (B, P, width)
```

![[Pasted image 20250826183646.png]]

VisionTransformer 使用标准方法，将可学习的分类token添加到补丁嵌入中，以执行分类。

```python
self.cls_token = nn.Parameter(torch.randn(1, 1, width))
```

批处理中的每个图像都需要具有分类token，因此我们将使用 `expand` 函数来使用 `self.cls_token` 为批处理中的每个图像创建分类token。

```python
x = torch.cat((self.cls_token.expand(x.size()[0], -1, -1),x), dim=1)
```

添加分类token后，我们需要将位置编码添加到嵌入中。

```python
x = self.positional_embedding(x)
```

添加位置编码后，我们现在可以将嵌入传递到编码器层中。

```python
# Transformer Encoder
for encoder_layer in self.encoder:
    x = encoder_layer(x)
```

从编码器层的输出中，我们只需要来自学习的分类token的信息。

```python
# Getting Class Tokens
x = x[:, 0, :]
```

最后，通过获取特征与学习投影之间的点积，将图像特征嵌入到联合嵌入空间中，并通过除以归一化的点积对其进行归一化。

```python
if self.projection is not None:
    x = x @ self.projection

x = x / torch.norm(x, dim=-1, keepdim=True)

return x
```

## CLIP 模型

```python
class CLIP(nn.Module):
    def __init__(self, emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers):
        super().__init__()
        self.image_encoder = ImageEncoder(vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, emb_dim)
        self.text_encoder = TextEncoder(vocab_size, text_width, max_seq_length, text_heads, text_layers, emb_dim)
        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


    def forward(self,image,text, mask=None):
        I_e = self.image_encoder(image)
        T_e = self.text_encoder(text, mask=mask)

        # scaled pairwise cosine similarities [n, n]
        logits = (I_e @ T_e.transpose(-2,-1)) * torch.exp(self.temperature)

        # symmetric loss function
        labels = torch.arange(logits.shape[0]).to(self.device)

        loss_i = nn.functional.cross_entropy(logits.transpose(-2,-1), labels)
        loss_t = nn.functional.cross_entropy(logits, labels)

        loss = (loss_i + loss_t) / 2

        return loss
```

当给定一批图像和标题时，CLIP 应该告诉我们哪些标题与哪些图像搭配。它通过一起训练文本和图像编码器来最大化应该在一起的对的成对余弦相似度分数，并最小化不应该在一起的对来做到这一点。

为此，我们首先需要从图像和文本编码器中获取嵌入式特征。

```python
def forward(self,image,text, mask=None):
  I_e = self.image_encoder(image)
  T_e = self.text_encoder(text, mask=mask)
```

使用嵌入的特征，我们可以通过使用嵌入的图像特征和嵌入文本特征的转置版本之间的点积来计算缩放的成对余弦相似度。余弦相似度应沿图中正确的图像和文本配对在一起的对角线最大化。

![[Pasted image 20250826184405.png]]

```python
logits = (I_e @ T_e.transpose(-2,-1)) * torch.exp(self.temperature)
```

这在 $I_e$ 和 $T_e$ 分别包含 $N$ 个批次时工作，从而产生上图所示的矩阵。为了最大化相关图像之间的余弦相似度，CLIP 使用对称/对比损失。我们可以通过首先创建与批次中的每条数据相对应的标签来计算此损失。

```python
# symmetric loss function
labels = torch.arange(logits.shape[0]).to(self.device)
```

然后，我们计算沿 logit 行的交叉熵损失，以获得图像的损失。

```python
loss_i = nn.functional.cross_entropy(logits.transpose(-2,-1), labels)
```

文本的损失是通过计算沿列的交叉熵损失来计算的。

```python
loss_t = nn.functional.cross_entropy(logits, labels)
```

我们通过计算图像损失和文本损失之间的平均值来获得最终损失。

```python
loss = (loss_i + loss_t) / 2

return loss
```

### 数据

```python
class FashionMNIST(Dataset):
    def __init__(self, train=True):
        self.dataset = load_dataset("fashion_mnist")
        self.transform = T.ToTensor()
        if train:
            self.split = "train"
        else:
            self.split = "test"


        self.captions = {0: "An image of a t-shirt/top",
                        1: "An image of trousers",
                        2: "An image of a pullover",
                        3: "An image of a dress",
                        4: "An image of a coat",
                        5: "An image of a sandal",
                        6: "An image of a shirt",
                        7: "An image of a sneaker",
                        8: "An image of a bag",
                        9: "An image of an ankle boot"}

    def __len__(self):
        return self.dataset.num_rows[self.split]

    def __getitem__(self,i):
        img = self.dataset[self.split][i]["image"]
        img = self.transform(img)

        cap, mask = tokenizer(self.captions[self.dataset[self.split][i]["label"]])

        mask = mask.repeat(len(mask),1)

        return {"image": img, "caption": cap, "mask": mask}
```

在本教程中，我们将使用 HuggingFace 中的 Fashion MNIST 数据集。我们选择这个数据集是因为它相当小并且保持训练时间合理。

```python
self.dataset = load_dataset("fashion_mnist")
```

对于数据集中的每个条目，我们将需要三样东西：图像、标题和文本掩码。

对于图像，我们唯一需要进行的更改是将图像转换为张量。

```python
img = self.dataset[self.split][i]["image"]
img = self.transform(img)
```

对于标题，我们需要将其传递给我们创建的分词器，以获取 token 表示以及 token 的掩码。

```python
cap, mask = tokenizer(self.captions[self.dataset[self.split][i]["label"]])
```

我们从分词器获得的掩码是大小为 `max_seq_length` 的 1D 张量。在文本编码器中，掩码将应用于形状为 `(max_seq_length, max_seq_length)` 的注意力分数。因此，我们需要扩展掩码，以便将其应用于注意力分数的每一行。

![[Pasted image 20250826185601.png]]

图像、标题和掩码作为字典保存在数据集中。

```python
return {"image": img, "caption": cap, "mask": mask}
```

### 训练参数

```python
emb_dim = 32
vit_width = 9
img_size = (28,28)
patch_size = (14,14)
n_channels = 1
vit_layers = 3
vit_heads = 3
vocab_size = 256
text_width = 32
max_seq_length = 32
text_heads = 8
text_layers = 4
lr = 1e-3
epochs = 10
batch_size = 128
```

### 加载数据集

```python
train_set = FashionMNIST(train = True)
test_set = FashionMNIST(train = False)

train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)
test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)
```

### 训练

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device: ", device, f"({torch.cuda.get_device_name(device)})" if torch.cuda.is_available() else "")

model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)

optimizer = optim.Adam(model.parameters(), lr=lr)

best_loss = np.inf
for epoch in range(epochs):
    for i, data in enumerate(train_loader, 0):
        img, cap, mask = data["image"].to(device), data["caption"].to(device), data["mask"].to(device)
        loss = model(img,cap,mask)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch [{epoch+1}/{epochs}], Batch Loss: {loss.item():.3f}")

    # Saves model if it performed better than the previous best
    if loss.item() <= best_loss:
        best_loss = loss.item()
        torch.save(model.state_dict(), "/content/drive/MyDrive/clip.pt")
        print("Model Saved.")
```

### 测试

```python
# Loading Best Model
model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)
model.load_state_dict(torch.load("/content/drive/MyDrive/clip.pt", map_location=device))

# Getting dataset captions to compare images to
text = torch.stack([tokenizer(x)[0] for x in test_set.captions.values()]).to(device)
mask = torch.stack([tokenizer(x)[1] for x in test_set.captions.values()])
mask = mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)

correct, total = 0,0
with torch.no_grad():
    for data in test_loader:
        images, labels = data["image"].to(device), data["caption"].to(device)
        image_features = model.image_encoder(images)
        text_features = model.text_encoder(text, mask=mask)

        image_features /= image_features.norm(dim=-1, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        _, indices = torch.max(similarity,1)
        pred = torch.stack([tokenizer(test_set.captions[int(i)])[0] for i in indices]).to(device)
        correct += int(sum(torch.sum((pred==labels),dim=1)//len(pred[0])))
        total += len(labels)

print(f'\nModel Accuracy: {100 * correct // total} %')
```

我们通过获取模型训练的标题并将其与实际标题进行比较来测试模型。在训练时，我们使用了相同的标题模板（“A image of a（n） {class}”），因此这个测试阶段与任何其他图像分类器几乎相同。我们实现了大约 85% 的模型准确率。

### Zero-Shot 分类

```python
# Loading Best Model
model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)
model.load_state_dict(torch.load("/content/drive/MyDrive/clip.pt", map_location=device))


# Captions to compare images to
class_names =["t-shirt/top",
                        "trousers",
                        "pullover",
                        "dress",
                        "coat",
                        "sandal",
                        "shirt",
                        "sneaker",
                        "bag",
                        "ankle boot"]

text = torch.stack([tokenizer(x)[0] for x in class_names]).to(device)
mask = torch.stack([tokenizer(x)[1] for x in class_names])
mask = mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)

idx = 1000

img = test_set[idx]["image"][None,:]
plt.imshow(img[0].permute(1, 2, 0)  ,cmap="gray")
plt.title(tokenizer(test_set[idx]["caption"], encode=False, mask=test_set[idx]["mask"][0])[0])
plt.show()
img = img.to(device)
with torch.no_grad():
  image_features = model.image_encoder(img)
  text_features = model.text_encoder(text, mask=mask)


image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
values, indices = similarity[0].topk(5)

# Print the result
print("\nTop predictions:\n")
for value, index in zip(values, indices):
    print(f"{class_names[int(index)]:>16s}: {100 * value.item():.2f}%")
```

对于zero-shot分类，我们将图像与类别的名称进行比较。我们输入标签以与图像进行比较，它将返回前 5 个预测以及预测的可能性。这不是 CLIP 执行zero-shot分类的最佳示例。使用 Fashion-MNIST 数据集使模型易于训练，但标题不是很丰富。要真正理解 CLIP 的zero-shot功能，包含多个名称的训练集会更合适。真正的zero-shot检测将允许检测以前未见过的排列。

![[Pasted image 20250826185937.png]]

