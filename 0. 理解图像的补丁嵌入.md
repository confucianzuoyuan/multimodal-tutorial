
```ad-note
patch：补丁
```

Transformer 架构如此强大，是因为它不会区分文本、图像、任何数据及其组合。“注意力”模型计算序列中每个token之间的自相关性，从而能够汇总和生成任何类型的数据。Vision Transformer通过将图像分解成二维补丁来实现这一点，然后将其展平为单个嵌入向量。此时，它们可以像文本嵌入（或任何其他嵌入）一样被处理，甚至可以与其他数据类型拼接。通常，创建补丁的步骤会与使用二维卷积的第一个可学习的非线性变换相结合，这部分不太好理解。本文将深入探讨这一步骤。

## 数据使用MNIST

```python
from torchvision.datasets.mnist import MNIST
from torch.utils.data import DataLoader
import torchvision.transforms as T
import torch
# 设定随机数，方便过程的复现
torch.manual_seed(42)

img_size = (32,32) # 将图片缩放到 32x32 的分辨率
batch_size = 4

transform = T.Compose([
    T.ToTensor(),
    T.Resize(img_size)
])

train_set = MNIST(
    root="./../datasets", train=True, download=True, transform=transform
)

train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)

batch = next(iter(train_loader)) # 加载第一个批次
```

可以使用 `matplotlib` 将由4幅图像和4个标签组成的批次可视化：

```python
import matplotlib.pyplot as plt

# batch[0] 包含图像，batch[1] 包含标签
images = batch[0]
labels = batch[1]

# 为子图创建画布和坐标轴
fig, axes = plt.subplots(1, batch_size, figsize=(12, 4))

# 遍历批次中的图像和标签
for i in range(batch_size):
    # 将图像张量转换为NumPy数组，如果是灰度图像则移除通道维度
    image_np = images[i].numpy().squeeze()

    # 在对应的子图中显示图像
    axes[i].imshow(image_np, cmap='gray')  # 对灰度图像使用'gray'颜色映射
    axes[i].set_title(f"类别: {labels[i].item()}") # 假设标签是张量，使用.item()获取数值
    axes[i].axis('off')

# 调整子图之间的间距
plt.tight_layout()

# 显示图像
plt.show()
```

## 创建图像的补丁

使用Transformer神经网络处理图像的第一步是将其分解成多个patch（补丁）。在本例中，我们可以将32x32的图像分解成64个4x4的补丁、16个8x8的补丁或8个16x16的补丁：

![[补丁示例.png]]

虽然我们以二维的形式显示这些补丁，但我们也可以将它们存储在维度为 16、64 或 256 的列向量中。此时，它们已经与文本嵌入无法区分，并且它们的序列与字符串或单词完全相同。

以下是使用 Torch 的 `unfold` 运算符分解图像的代码：

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

# Image and patch sizes
img_size = (32, 32)
patch_size = (8, 8)
n_channels = 1

image = batch[0][1].unsqueeze(0)

# Patch Class
class Patch(nn.Module):
    def __init__(self, img_size, patch_size, n_channels):
        super().__init__()
        self.patch_size = patch_size
        self.n_channels = n_channels

    def forward(self, x): # B x C x H X W
        x = x.unfold(
                2, self.patch_size[0], self.patch_size[0]
            ).unfold(
                3, self.patch_size[1], self.patch_size[1]
            )  # (B, C, P_row, P_col, P_height, P_width)
        x = x.flatten(2)  #(B, C, P_row * P_col * P_height * P_width)
        x = x.transpose(1, 2)  # (B, P_row * P_col * P_height * P_width, C)
        return x

# Instantiate model
patch = Patch(img_size, patch_size, n_channels)

# Extract patches
with torch.no_grad():
    patches = patch(image) 

# Visualize
patches = patches.squeeze(0)  # Remove batch dimension -> (P, d_model)
patches = patches.view(-1, patch_size[0], patch_size[1]) # reshape back into 8x8

npatches = img_size[0] // patch_size[0]
# Plot patches
fig, axs = plt.subplots(npatches, npatches, figsize=(6, 6))  # 4x4 grid for (32x32) -> 16 patches

for i in range(npatches):
    for j in range(npatches):
        patch_idx = i * npatches + j  # Patch index
        axs[i, j].imshow(patches[patch_idx], cmap="gray", vmin=0, vmax=1)
        axs[i, j].axis("off")

plt.show()
```

上面的代码，运算方式是先沿高度维度展开，然后沿宽度维度展开。所有操作的维度均在注释中显示，其中 `B` 代表批次， `C` 代表通道数（本例中为1）， `H` 代表高度， `W` 代表宽度。展开后，我们将张量从存储图像数据的第二维开始展平，最后进行转置，使颜色通道位于最后。

代码的其余部分实例化该类，转换图像并将其可视化。请注意，我们需要删除批次维度，然后将一维图像数据转换回二维张量才能正确显示它们。

## 创建补丁嵌入

![[Pasted image 20250903105156.png]]

==使用单位矩阵进行线性变换后的补丁（左）、使用随机权重进行线性变换后的补丁（中）以及使用随机权重和偏置进行线性变换后的补丁。==

此外，为了便于可视化，这些嵌入已被转换回二维张量，并展示了线性投影如何在每个补丁中运行。使用单位矩阵作为权重初始化 `nn.Linear` 类，可以发现原始数据得以保留。使用随机权重，我们可以看到图像中值为零的部分保持不变。最后，我们可以添加一个偏置，以表明变换确实对每个补丁的影响相同——所有空补丁都显示出完全相同的偏置。

下面是一个新的类，现在称为 `PatchEmbedding` ，并附有一行代码来实例化它。请注意，我们引入了新变量 `d_model` ，它是输出嵌入所需的维度。现在可以是任意数字。我们在这里选择 `d_model=64` ，因为这是上图的设置，但现在不再有限制了。

```python
class PatchEmbedding(nn.Module):
    def __init__(self, img_size, patch_size, n_channels, d_model):
        super().__init__()
        self.patch_size = patch_size
        self.n_channels = n_channels
        self.d_model = d_model

        # Linear projection layer to map each patch to d_model
        self.linear_proj = nn.Linear(patch_size[0] * patch_size[1] * n_channels, d_model,bias=False)
        # The next two lines are unnecessary, but help to visualize that the linear 
        # projection operates along the correct dimensions
        #with torch.no_grad():
        #  self.linear_proj.weight.copy_(torch.eye(self.linear_proj.weight.shape[0]))
       
    def forward(self, x): # B x C x H X W
        x = x.unfold(
                2, self.patch_size[0], self.patch_size[0]
            ).unfold(
                3, self.patch_size[1], self.patch_size[1]
            )  # (B, C, P_row, P_col, P_height, P_width)
        
        B, C, P_row, P_col, P_height, P_width = x.shape
        x = x.reshape(B, C, P_row * P_col, P_height * P_width)
        x = self.linear_proj(x)  # (B*N, d_model)
        
        x = x.flatten(2)  # (B, C, P_row * P_col * P_height * P_width)
        x = x.transpose(1, 2)  # (B, P_row * P_col * P_height * P_width, C)
        x = x.view(B, -1, self.d_model)
        return x

d_model = 64
# Instantiate model
patch = PatchEmbedding(img_size, patch_size, n_channels, d_model)
```

事实上，只要二维的，我们仍然可以将结果可视化，如下所示 `d_model=2` 和 `d_model=2500` 的输出：

![[Pasted image 20250903105605.png]]

==嵌入到 4 维（左）和 2500 维（右）向量后的数字“2”的图像。==

我们可以看到，非线性变换，一个全连接神经网络，将 8x8（64）的输入转换为 `d_model` ，可以包含相当多的可学习参数，从左侧的 64x4（256）到右侧的 64x2500（160k）。你可以使用以下命令自行测试：

```python
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

count_parameters(patch)
```

## 使用二维卷积创建补丁嵌入

你可能已经注意到， `unfold` 运算符相当笨拙，甚至有点令人恼火。有一种更简单的方法可以将展开运算符和线性变换结合起来，即使用与所需补丁大小对应的核大小和步长执行二维卷积。这样，卷积就不是逐像素操作，而是逐补丁操作，从而得到与 `unfold` 运算符和 `nn.Linear` 结合使用时相同的结果：

![[Pasted image 20250903105817.png]]

==使用二维卷积将创建补丁和线性变换结合在一个步骤中。16x16 补丁嵌入到 4、64 和 2500 维（顶行），8x8 补丁嵌入到 4、64 和 2500 维（底行）。==

这是修改后的 `PatchEmbedding` 类：

```python
class PatchEmbedding(nn.Module):
    def __init__(self, img_size, patch_size, n_channels, d_model):
        super().__init__()
        self.patch_size = patch_size
        self.n_channels = n_channels
        self.d_model = d_model  # Flattened patch size

        # Conv2d to extract patches
        self.linear_project = nn.Conv2d(
            in_channels=n_channels,
            out_channels=self.d_model,  # Each patch is flattened to d_model
            kernel_size=patch_size,
            stride=patch_size,
            bias=False
        )

    def forward(self, x):
        x = self.linear_project(x)  # (B, d_model, P_row, P_col)
        x = x.flatten(2)  # (B, d_model, P_row * P_col) -> (B, d_model, P)
        x = x.transpose(1, 2)  # (B, P, d_model)
        return x
```

请注意，可以将上述任何图像补丁嵌入输入到 Vision Transformer 中。使用二维卷积是最通用且最紧凑的表示方式。需要注意的是，卷积每个维度使用一个专用核，而到目前为止，我们一直对每个图像补丁使用相同的核。

我们可以通过初始化卷积核权重来说明这一点，并测试卷积不会产生任何异常，这样每个卷积核在每个补丁中只提取一个像素。以下代码适用于补丁大小 `(8,8)` 和生成的 `d_model=64` 。将其添加到 `PatchEmbedding` 类的 `__init__` 方法末尾：

```python
        """Initialize Conv2d to extract patches without transformation."""
        with torch.no_grad():
            identity_kernel = torch.zeros(
                self.d_model, self.n_channels, *self.patch_size
            )  # Shape: (64, 1, 8, 8)

            for i in range(self.d_model):  
                row = i // self.patch_size[1]  # Row index in the patch
                col = i % self.patch_size[1]   # Column index in the patch
                identity_kernel[i, 0, row, col] = 1  # Place a 1 at the correct pixel position

            self.linear_project.weight.copy_(identity_kernel)
```

`identity_kernel` 张量维护 `d_model` 个条目，每个维度一个，并且每个 `patch` 中只有一个像素被设置为 1，从而只提取该像素。一种更简单的方法是将 `d_model x d_model` 单位矩阵简单地转换为 `patch_size` 的 `d_model` 矩阵：

```python
        identity_matrix = torch.eye(self.d_model)
        identity_kernel = identity_matrix.view(d_model, 1, *patch_size)  # Shape: (64, 1, 8, 8)

        with torch.no_grad():
             self.linear_project.weight.copy_(identity_kernel)
```

两种方法的结果相同，但第一种方法更清楚地表明了实际情况：每个核都是一个零矩阵，只有一个元素为 1。

无论你使用的是线性变换还是一组小核，它们的参数数量都相同。你可以通过检查两个补丁嵌入的数据结构来看到这一点：

```python
PatchEmbedding(
  (linear_proj): Linear(in_features=64, out_features=64, bias=False)
)
```

和

```python
PatchEmbedding(
    (linear_project): Conv2d(1, 64, kernel_size=(8, 8), stride=(8, 8), bias=False)
)
```

其中一个只是一个 `64x64` 矩阵（4096 个参数）。另一个由 64 个 `8x8` 矩阵组成，同样包含 4096 个参数。

了解了将图像分解成补丁嵌入序列的不同方式后，现在可以通过Vision Transformer来使用它们了。

