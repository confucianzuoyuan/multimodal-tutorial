## 背后的数学理论

### 概述

![[Pasted image 20250903163226.png]]

扩散模型的训练可以分为两部分：

1. 正向扩散过程→给图像添加噪声。
2. 反向扩散过程→从图像中去除噪声。

### 正向扩散过程

![[Pasted image 20250903163405.png]]

![[3.1.excalidraw|10000]]

前向扩散过程逐步将高斯噪声添加到输入图像 $x_0$ 中，总共会有 $T$ 步。该过程将产生一系列带噪声的图像样本 $x_1, \dots, x_T$ 。

当 $T → ∞$ 时，最终结果将变成完全噪声图像，就像从 ==各向同性== 的高斯分布中采样出来的噪声一样。

但我们不需要设计一种算法来迭代地向图像中添加噪声，而是可以使用闭式公式（解析解）在特定的时间步长 $t$ 直接对噪声图像进行采样。

#### 闭式公式（解析解）

可以使用 **重参数化技巧** 推导出解析形式的采样公式。

首先，如果 $z\sim\mathcal{N}(\mu,\sigma^2)$ 的话，那么有下面的结论

$$
z = \mu + \sigma\epsilon \quad\text{其中}\epsilon\sim\mathcal{N}(0, 1)
$$

利用这个技巧，我们可以将采样图像 $x_t$ 表示如下：

$$
x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}\epsilon_{t-1}
$$

==然后我们可以递归地展开它来得到闭合形式的公式：==

![[重参数技巧公式.png]]

```ad-tip
所有 $\epsilon$ 都是 i.i.d.（独立同分布）标准正态随机变量。

使用不同的符号和下标来区分它们非常重要，因为它们是独立的，并且在采样后它们的值可能会有所不同。
```

但是我们如何从第 4 行跳到第 5 行呢？

![[Pasted image 20250903171649.png|600]]

有些人觉得这个步骤比较难理解。我们推导一下：

![[3.2.excalidraw|1000]]

我们用 $X$ 和 $Y$ 来表示这两个项。它们可以被视为来自两个不同正态分布的样本。即

$$
X\sim\mathcal{N}(0,\alpha_t(1-\alpha_{t-1})I)
$$

和

$$
Y\sim\mathcal{N}(0,(1-\alpha_{t})I)
$$

回想一下，两个正态分布（独立）随机变量的和也是正态分布的。即，如果 $Z=X+Y$ ，那么有下面的公式

$$
Z\sim\mathcal{N}(0, \sigma^2_X+\sigma^2_Y)
$$

因此，我们可以将它们合并在一起，并以重新参数化的形式表示合并后的正态分布。这就是我们将这两个项合并的方法。

重复这些步骤将为我们提供以下仅取决于输入图像 $x_0$ 的公式：

$$
x_t = \sqrt{\overline{\alpha}_t}x_{0}+\sqrt{1-\overline{\alpha}_t}\epsilon
$$

现在我们可以使用此公式在任何时间步骤直接对 $x_t$ 进行采样，这使得前向过程更快。

### 反向扩散过程

![[3.3.excalidraw|10000]]

与前向过程不同，我们不能使用 $q(x_{t-1}|x_t)$ 来逆转噪声，因为它是难以处理的（不可计算的）。

```ad-tip
给定一张纯噪声图片（$x_t$）为条件，图片（$x_0$）是一只猫的概率是多少？无法计算。
```

因此，我们需要训练一个神经网络 $p_\theta(x_{t-1}|x_t)$ 来近似 $q(x_{t-1}|x_t)$ 。近似值 $p_\theta(x_{t-1}|x_t)$ 服从正态分布，其均值和方差如下：

$$
\begin{cases}
\mu_\theta(x_t, t) &:= \tilde{\mu}_t(x_t, x_0) \\
\Sigma_\theta(x_t, t) &:= \tilde{\beta}_t I
\end{cases}
$$

#### 损失函数

我们可以将损失定义为负对数似然：

![[3.4.excalidraw|10000]]

这里我们可以优化变分下界，而不是优化难以解决的损失函数本身。

![[3.5.excalidraw|1000]]

通过优化可计算的下界函数，我们可以间接优化难以解决的损失函数。

![[3.6.excalidraw|10000]]

==变分下界的推导与扩展==

通过扩展变分下界，我们发现它可以用以下三个项表示：

**1. 常数项** ：$L_T$

由于 $q$ 没有可学习的参数，而 $p$ 只是高斯噪声概率，因此该项在训练期间将是一个常数，因此可以忽略不计。

**2. 逐步去噪项** ：$L_{t-1}$

该项将目标去噪步骤 $q$ 与近似去噪步骤 $p_θ$ 进行比较。

```ad-tip
请注意，通过对 $x_0$ 进行条件分析，$q(x_{t-1}|x_t,x_0)$ 变得容易处理。
```

![[逐步去噪项公式推导.png]]

经过一系列推导，平均值 $\tilde{\mu}_t$ 的 $q(x_{t-1}|x_t,x_0)$ 如上所示。

平均值的推导过程见后面。

为了近似目标去噪步长 $q$ ，我们只需使用神经网络近似其均值即可。因此，我们将近似均值 $\mu_\theta$ 设置为与目标均值 $\tilde{\mu}_t$ 相同的形式（使用可学习的神经网络 $\epsilon_\theta$ ）：

![[Pasted image 20250903183630.png|600]]

可以使用均方误差（MSE）来比较目标均值和近似均值：

![[Pasted image 20250903183650.png]]

实验表明，忽略加权项并简单地计算目标噪声和预测噪声的 MSE 可以获得更好的结果。

![[3.11.excalidraw|10000]]


**3. 重建项** ：$L_0$

这是最后一步去噪的重建损失，由于以下原因，可以在训练期间忽略它：

- 它可以使用 $L_{t-1}$ 中的相同神经网络来近似。
- 忽略它会使样本质量更好，并且使其更易于实现。

#### 简化的损失函数

因此最终简化的训练目标如下：

![[简单损失函数.excalidraw|10000]]

### U-Net模型

#### 数据集

在每个 Epoch：

1. 将为每个训练样本（图像）选择一个随机时间步长 $t$ 。
2. 对每幅图像应用高斯噪声（对应于 $t$ ）。
3. 将时间步长转换为嵌入（向量）。

![[Pasted image 20250903184137.png]]

#### 训练

```ad-danger
title: 训练算法伪代码

1. **Repeat**
2. $\quad$ $\mathbf{x}_{0}\sim q(\mathbf{x}_{0})$              # 从数据集中抽取一张图片
3. $\quad$ $t\sim \text{Uniform}(\{1,\dots,T\})$               # 从均匀分布中采样一个时间步
4. $\quad$ $\epsilon\sim\mathcal{N}(0,\mathbf{I})$             # 从正态分布中采样一个噪声
5. $\quad$ 使用梯度下降法，梯度为
$$
\nabla_{\theta}\left\Vert{ \epsilon - \epsilon_{\theta}\left(\sqrt{ \overline{\alpha}_{t} }\mathbf{x}_{0}+\sqrt{ 1-\overline{\alpha}_{t} }\epsilon,t\right) }\right\Vert^{2}
$$
6. **Until** 收敛
```

官方的训练算法如上，下图是一个训练步骤的示意图：

![[扩散模型训练步骤示意图.excalidraw|10000]]

#### 反向扩散

```ad-danger
title: 采样算法

1. $\mathbf{x}_T\sim\mathcal{N}(\mathbf{0},\mathbf{I})$
2. **for** $t=T,...,1$ **do**
3. $\quad$ $\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$ **if** $t>1$, **else** $\mathbf{z}=\mathbf{0}$
4. $\quad$ $\mathbf{x}_{t-1}=\frac{1}{\sqrt{ \alpha_{t} }}\left( \mathbf{x}_{t}-\frac{1-\alpha_{t}}{\sqrt{  1-\overline{\alpha}_{t}}}\epsilon_{\theta}(\mathbf{x}_{t},t) \right) + \sigma_{t}\mathbf{z}$
5. **end for**
6. **return** $\mathbf{x}_0$
```

我们可以使用上述算法从噪声中生成图像。下图是它的说明：

![[扩散模型采样示意图.excalidraw|10000]]

请注意，在最后一步，我们只是输出学习到的平均值 $\mu_\theta(x_1,1)$ ，而不向其中添加噪声。

### 平均值 $\tilde{\mu}_t$ 的详细推导（选学）

以下是损失函数部分中逐步去噪项中 $q(x_{t-1}|x_t,x_0)$ 的平均值 $\tilde{\mu}_t$ 的详细推导。

![[均值推导过程.png]]
